{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27192f",
   "metadata": {
    "id": "6c27192f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14efbb6",
   "metadata": {
    "id": "f14efbb6"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56819b49-b3bb-4d32-ad88-a458e7ee142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ff42b4-0dd7-4555-bfbc-c1d6a7c65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Product = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42efd20-8c8f-4675-aa1a-9a2534ee60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_percentage = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce9a174-68e6-47f3-ad2f-966cc2698e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc24047e-fa23-40ae-b761-5f0d1fbe595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338e9f4b-a5a7-47a9-b06d-5216d077594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9083308b",
   "metadata": {
    "id": "9083308b"
   },
   "outputs": [],
   "source": [
    "# Set constants\n",
    "USER_Cont_FEATURES = 2*user_continuous_feature_multiplier\n",
    "USER_Dicr_FEATURES = 3\n",
    "\n",
    "Product_Cont_FEATURES = 3*prod_continuous_feature_multiplier\n",
    "Product_Dicr_FEATURES = 2\n",
    "OUTSIDE_OPTION_UTILITY = 0\n",
    "utilities = torch.zeros(NUM_USER, NUM_Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442d9dd4",
   "metadata": {
    "id": "442d9dd4"
   },
   "outputs": [],
   "source": [
    "def generate_features(N, C, D):\n",
    "    continuous_features = np.zeros((N, C))\n",
    "    for i in range(C):\n",
    "        continuous_features[:, i] = np.random.uniform(0,1,size=N)\n",
    "    binary_features = np.random.randint(0, 2, (N, D))\n",
    "    return np.hstack((continuous_features, binary_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb84b940",
   "metadata": {
    "id": "cb84b940"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class UtilityDNN(nn.Module):\n",
    "    def __init__(self, user_features, product_features):\n",
    "        super(UtilityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features + product_features, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PriceSensitivityDNN(nn.Module):\n",
    "    def __init__(self, user_features):\n",
    "        super(PriceSensitivityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "\n",
    "\n",
    "        nn.init.uniform_(self.fc1.weight, a=-0.5, b=0.5)\n",
    "        nn.init.uniform_(self.fc2.weight, a=-0.5, b=0.5)\n",
    "        nn.init.uniform_(self.fc3.weight, a=-0.5, b=0.5)\n",
    "        nn.init.uniform_(self.fc4.weight, a=-0.5, b=0.5)\n",
    "        nn.init.uniform_(self.fc1.bias, a=-0.5, b=0.5)\n",
    "        nn.init.uniform_(self.fc2.bias, a=-0.5, b=0.5)\n",
    "        nn.init.uniform_(self.fc3.bias, a=-0.5, b=0.5)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return torch.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e26f6da",
   "metadata": {
    "id": "0e26f6da"
   },
   "outputs": [],
   "source": [
    "def utility_model(x_user, X_product, price, user_randomization, prod_randomization,pair_utility_model,price_sensitivity_model,gumbel_noise):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    \n",
    "\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    for i in range(num_users):\n",
    "\n",
    "        for j in range(num_products):\n",
    "            # Determine if the user and product are in the treatment group\n",
    "            is_user_treated = (user_randomization[i] == 1)\n",
    "            is_product_treated = (prod_randomization[j] == 1)\n",
    "\n",
    "            # Adjust price based on the experiment conditions\n",
    "            adjusted_price = price[j] * discount if is_user_treated or is_product_treated else price[j]\n",
    "            combined_features = torch.cat((x_user[i], X_product[j]), 0)\n",
    "            utility_from_dnn = pair_utility_model(combined_features)\n",
    "            price_effect = price_sensitivities[i] * adjusted_price\n",
    "\n",
    "            utilities[i, j] = utility_from_dnn - price_effect + gumbel_noise[i,j]\n",
    "\n",
    "    return utility_from_dnn,price_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22bf01fa",
   "metadata": {
    "id": "22bf01fa"
   },
   "outputs": [],
   "source": [
    "def make_decision(utilities):\n",
    "    num_users = utilities.shape[0]\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  \n",
    "    for i in range(num_users):\n",
    "        max_utility, chosen_product = torch.max(utilities[i], dim=0)\n",
    "\n",
    "        # Compare the maximum utility with the outside option (utility = 0)\n",
    "        if max_utility <= 0:\n",
    "            decisions[i] = -1 \n",
    "        else:\n",
    "            decisions[i] = chosen_product \n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7b9061",
   "metadata": {
    "id": "3a7b9061"
   },
   "outputs": [],
   "source": [
    "def calculate_revenue(decisions, prices):\n",
    "    total_revenue = 0.0\n",
    "\n",
    "    # Iterate over each decision and add the corresponding product price to total revenue\n",
    "    for i, decision in enumerate(decisions):\n",
    "        if decision != -1:  # Check if the decision is not the outside option\n",
    "            total_revenue += prices[decision].item()  # Add the price of the chosen product\n",
    "\n",
    "    return total_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be800ea9",
   "metadata": {
    "id": "be800ea9"
   },
   "outputs": [],
   "source": [
    "X_user = generate_features(NUM_USER,USER_Cont_FEATURES, USER_Dicr_FEATURES)\n",
    "X_product = generate_features(NUM_Product, Product_Cont_FEATURES, Product_Dicr_FEATURES)\n",
    "price = np.random.uniform(0.5 ,1, NUM_Product)\n",
    "\n",
    "X_user = torch.from_numpy(X_user).float()\n",
    "X_product = torch.from_numpy(X_product).float()\n",
    "price = torch.from_numpy(price).float()\n",
    "gumbel_dist = torch.distributions.Gumbel(0, 1)\n",
    "gumbel_noise = gumbel_dist.sample((NUM_USER, NUM_Product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13b1ab05",
   "metadata": {
    "id": "13b1ab05"
   },
   "outputs": [],
   "source": [
    "pair_utility_model = UtilityDNN(X_user.shape[1], X_product.shape[1])\n",
    "price_sensitivity_model = PriceSensitivityDNN(X_user.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a148e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def utility_model_batched(x_user, X_product, price, user_randomization, prod_randomization, pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  # Initialize decision array\n",
    "\n",
    "    # Convert numpy arrays to tensors if necessary\n",
    "    if isinstance(user_randomization, np.ndarray):\n",
    "        user_randomization = torch.from_numpy(user_randomization).to(torch.bool)\n",
    "    if isinstance(prod_randomization, np.ndarray):\n",
    "        prod_randomization = torch.from_numpy(prod_randomization).to(torch.bool)\n",
    "    if isinstance(price, np.ndarray):\n",
    "        price = torch.from_numpy(price)\n",
    "\n",
    "    # Compute price sensitivities outside the batch loop\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    # Iterate over users in batches\n",
    "    for i in range(0, num_users, batch_size):\n",
    "        batch_end = min(i + batch_size, num_users)  # Define the end of the batch\n",
    "        batch_indices = slice(i, batch_end)  # Slice for batch indexing\n",
    "\n",
    "        # Repeat the product features and price for each user in the batch\n",
    "        batch_user_features = x_user[batch_indices].unsqueeze(1).expand(-1, num_products, -1)\n",
    "        batch_prod_features = X_product.unsqueeze(0).expand(batch_end - i, -1, -1)\n",
    "        batch_price = price.unsqueeze(0).expand(batch_end - i, -1)\n",
    "\n",
    "        # Handle treatment adjustments in batch\n",
    "        batch_user_treatment = user_randomization[batch_indices].unsqueeze(1).expand(-1, num_products) == 1\n",
    "        batch_prod_treatment = prod_randomization.unsqueeze(0).expand(batch_end - i, -1) == 1\n",
    "        batch_adjusted_price = torch.where(batch_user_treatment | batch_prod_treatment, batch_price * discount, batch_price)\n",
    "\n",
    "        # Combine user and product features\n",
    "        combined_features = torch.cat((batch_user_features, batch_prod_features), dim=2)\n",
    "\n",
    "        # Compute utilities using the neural network in a batch\n",
    "        utility_from_dnn = pair_utility_model(combined_features.view(-1, combined_features.shape[-1])).view(batch_end - i, num_products)\n",
    "        price_effect = price_sensitivities[batch_indices] * batch_adjusted_price\n",
    "        batch_utilities = utility_from_dnn - price_effect + gumbel_noise[batch_indices]\n",
    "        max_utilities, chosen_products = torch.max(batch_utilities, dim=1)\n",
    "\n",
    "        # Compare the maximum utility with the outside option (utility = 0)\n",
    "        outside_option = -1 * torch.ones_like(chosen_products, dtype=torch.long)  # Match dimension and dtype\n",
    "        decisions[batch_indices] = torch.where(max_utilities > 0, chosen_products, outside_option)\n",
    "\n",
    "    return decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97381ec",
   "metadata": {
    "id": "c97381ec"
   },
   "source": [
    "# GTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c9652",
   "metadata": {
    "id": "c74c9652"
   },
   "source": [
    "## All treated scenario: all products are discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8620e2f7",
   "metadata": {
    "id": "8620e2f7"
   },
   "outputs": [],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a69429d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a69429d2",
    "outputId": "da77f021-0faf-424d-91be-088437608bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([8, 8, 8,  ..., 7, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "decisions_all_treat=utility_model_batched(X_user, X_product,price, user_randomization, prod_randomization, \n",
    "                                          pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3b7df52-fdc0-46a9-ad64-9119957d6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "all_num_unique = torch.unique(decisions_all_treat).numel()\n",
    "print(all_num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8748518-f99d-47ae-a031-7b89fbbfa669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(804)\n",
      "tensor(939)\n",
      "tensor(1149)\n",
      "tensor(1268)\n",
      "tensor(860)\n",
      "tensor(827)\n",
      "tensor(1195)\n",
      "tensor(896)\n",
      "tensor(1215)\n",
      "tensor(847)\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1,10):\n",
    "    print(torch.sum(decisions_all_treat==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f59995c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "f59995c9",
    "outputId": "b6b193fc-3ab4-4e76-8096-49dfeae0a46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue from sales when all products are discounted: $1494.85\n"
     ]
    }
   ],
   "source": [
    "total_revenue_all_treated = calculate_revenue(decisions_all_treat, price*discount)\n",
    "print(f\"Total revenue from sales when all products are discounted: ${total_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86abbc5",
   "metadata": {},
   "source": [
    "## All control scenario: all products remain the original price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34e8b6df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "34e8b6df",
    "outputId": "846dbf7a-920f-4c54-d79f-1b527ec86ff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([3, 8, 8,  ..., 7, 1, 3])\n",
      "Total Revenue from Sales: $7343.46\n"
     ]
    }
   ],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "\n",
    "decisions_all_control =utility_model_batched(X_user, X_product,price, user_randomization, prod_randomization, \n",
    "                                          pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_control)\n",
    "total_revenue_all_control = calculate_revenue(decisions_all_control, price)\n",
    "print(f\"Total Revenue from Sales: ${total_revenue_all_control:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e3c0837",
   "metadata": {
    "id": "2e3c0837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (ALLTreated - ALLControl): $-5848.61\n"
     ]
    }
   ],
   "source": [
    "revenue_difference = total_revenue_all_treated - total_revenue_all_control\n",
    "print(f\"Revenue Difference (ALLTreated - ALLControl): ${revenue_difference:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/total_revenue_all_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Sp6U7JOV1vEi",
   "metadata": {
    "id": "Sp6U7JOV1vEi"
   },
   "outputs": [],
   "source": [
    "true = revenue_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec3bca",
   "metadata": {
    "id": "d0ec3bca"
   },
   "source": [
    "## product randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cbbd718",
   "metadata": {
    "id": "4cbbd718"
   },
   "outputs": [],
   "source": [
    "def calculate_product_revenue(decisions, prices, prod_randomization):\n",
    "    revenue_treated = 0.0\n",
    "    revenue_control = 0.0\n",
    "\n",
    "    # Iterate over each user's decision\n",
    "    for user_index, decision in enumerate(decisions):\n",
    "        if decision != -1:  # If the user chose a product\n",
    "            product_price = prices[decision].item()  # Get the price of the chosen product\n",
    "\n",
    "            # Check if the product was in the treatment or control group\n",
    "            if prod_randomization[decision] == 1:\n",
    "                revenue_treated += product_price\n",
    "            else:\n",
    "                revenue_control += product_price\n",
    "\n",
    "    return revenue_treated, revenue_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "402ee3f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402ee3f8",
    "outputId": "17e270ca-c5e5-4e92-fd2f-2c675eb2fbf8"
   },
   "outputs": [],
   "source": [
    "utilities = torch.zeros(NUM_USER, NUM_Product)\n",
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1-treatment_percentage, treatment_percentage])\n",
    "# prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, ])\n",
    "decisions_product_randomization =utility_model_batched(X_user, X_product,price, user_randomization, prod_randomization, \n",
    "                                          pair_utility_model, price_sensitivity_model, gumbel_noise, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "022d9af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022d9af1",
    "outputId": "9f338893-e1f9-4c6a-945c-408a2e5b6f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue from Treated Products: $675.59\n",
      "Revenue from Control Products: $3828.56\n",
      "Revenue Difference (Treated - Control) by naive DIM: $-6305.93\n"
     ]
    }
   ],
   "source": [
    "revenue_treated, revenue_control = calculate_product_revenue(decisions_product_randomization, price-price*(1-discount)*prod_randomization, prod_randomization)\n",
    "naive = revenue_treated/treatment_percentage - revenue_control/(1-treatment_percentage)\n",
    "print(f\"Revenue from Treated Products: ${revenue_treated:.2f}\")\n",
    "print(f\"Revenue from Control Products: ${revenue_control:.2f}\")\n",
    "print(f\"Revenue Difference (Treated - Control) by naive DIM: ${naive:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/revenue_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b9eb",
   "metadata": {},
   "source": [
    "## Prepare training and testing data given experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "wUgBFRaYHK7-",
   "metadata": {
    "id": "wUgBFRaYHK7-"
   },
   "outputs": [],
   "source": [
    "X_user_1, X_user_2, decision_1, decision_2 = train_test_split(\n",
    "X_user, decisions_product_randomization, test_size=1/2, random_state=3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bd5975d-53cb-45c6-90e2-c36e313515a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\n",
    "    'features': X_user_1,\n",
    "    'labels': decision_1\n",
    "}\n",
    "\n",
    "test_set = {\n",
    "    'features': X_user_2,\n",
    "    'labels': decision_2\n",
    "}\n",
    "\n",
    "# Flag to switch between training and test set\n",
    "use_train_set = False  # Set to False for the test set\n",
    "\n",
    "# Function to get the current active dataset\n",
    "def get_active_dataset(use_train):\n",
    "    return train_set if use_train else test_set\n",
    "def get_test_dataset(use_train):\n",
    "    return test_set if use_train else train_set\n",
    "# Retrieve the current dataset based on the flag\n",
    "current_dataset = get_active_dataset(use_train_set)\n",
    "X_user_train = current_dataset['features']\n",
    "decision_train = current_dataset['labels']\n",
    "X_user_test = get_test_dataset(use_train_set)['features']\n",
    "decision_test =  get_test_dataset(use_train_set)['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320887a",
   "metadata": {
    "id": "d320887a"
   },
   "source": [
    "# use simple MNL structural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b14cd096",
   "metadata": {
    "id": "b14cd096"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearMNLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(LinearMNLModel, self).__init__()\n",
    "        # Initialize parameters for user and product features\n",
    "        self.beta_user = nn.Parameter(torch.randn(user_feature_dim))\n",
    "        self.beta_product = nn.Parameter(torch.randn(product_feature_dim))\n",
    "        self.beta_price = nn.Parameter(torch.tensor(-1.0)) \n",
    "\n",
    "    def forward(self, x_user, X_product, price, user_randomization, prod_randomization):\n",
    "        N, M = x_user.shape[0], X_product.shape[0]\n",
    "\n",
    "        # Expand user and product features to create a [N, M, F] shaped tensor for each\n",
    "        x_user_expanded = x_user.unsqueeze(1).expand(-1, M, -1).detach()\n",
    "        X_product_expanded = X_product.unsqueeze(0).expand(N, -1, -1).detach()\n",
    "\n",
    "\n",
    "        # Calculate linear utility from features\n",
    "        utility_user = torch.sum(x_user_expanded * self.beta_user, dim=2)\n",
    "        utility_product = torch.sum(X_product_expanded * self.beta_product, dim=2)\n",
    "\n",
    "        # Adjust prices based on randomization\n",
    "        adjusted_price = torch.where(\n",
    "             prod_randomization.unsqueeze(0),\n",
    "            price * discount,  \n",
    "            price\n",
    "        )\n",
    "\n",
    "        # Calculate utility from price, properly expanding its dimension\n",
    "        utility_price = adjusted_price * self.beta_price  # [M]\n",
    "        utility_price = utility_price.expand(N, M)  # [N, M]\n",
    "\n",
    "        # Total utility including features and price\n",
    "        total_utility = utility_user + utility_product + utility_price\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=total_utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities,total_utility), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d027617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89f614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearMNLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a74e297a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a74e297a",
    "outputId": "d9b4de3b-2c2d-452e-b491-1b217ba2aaf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.044724464416504\n",
      "Epoch 1000, Loss: 2.283658504486084\n",
      "Epoch 2000, Loss: 2.283026933670044\n",
      "Epoch 3000, Loss: 2.282896041870117\n",
      "Epoch 4000, Loss: 2.2828516960144043\n",
      "Epoch 5000, Loss: 2.282832145690918\n",
      "Epoch 6000, Loss: 2.282822608947754\n",
      "Epoch 7000, Loss: 2.28281831741333\n",
      "Epoch 8000, Loss: 2.2828164100646973\n",
      "Epoch 9000, Loss: 2.2828152179718018\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    utilities = model(X_user_train, X_product, price, user_randomization, prod_randomization)\n",
    "    choice_probabilities = nn.functional.log_softmax(utilities, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train+1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da3dbeb5-c326-49f2-95d1-948ed3fbb443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-23.0957,  -2.3702,  -2.1712,  ...,  -2.6283,  -2.2579,  -2.3147],\n",
       "        [-12.4211,  -2.3702,  -2.1712,  ...,  -2.6283,  -2.2579,  -2.3147],\n",
       "        [-24.2449,  -2.3702,  -2.1712,  ...,  -2.6283,  -2.2579,  -2.3147],\n",
       "        ...,\n",
       "        [-17.7212,  -2.3702,  -2.1712,  ...,  -2.6283,  -2.2579,  -2.3147],\n",
       "        [-18.3808,  -2.3702,  -2.1712,  ...,  -2.6283,  -2.2579,  -2.3147],\n",
       "        [-13.5658,  -2.3702,  -2.1712,  ...,  -2.6283,  -2.2579,  -2.3147]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f78d4b21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f78d4b21",
    "outputId": "6078e9c0-ce87-4f8e-dd3d-88c3cfb2a616"
   },
   "outputs": [],
   "source": [
    "beta_price_est = model.beta_price.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ac97d98-d381-4a84-82c7-aa6821c3aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6332691\n"
     ]
    }
   ],
   "source": [
    "print(beta_price_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e4df3c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e4df3c1",
    "outputId": "709d1125-67b8-459d-d512-a95764796126"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 5.2265, 10.7351,  6.6963,  0.6764,  7.3317], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beta_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1783523",
   "metadata": {
    "id": "a1783523"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17eb38d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17eb38d3",
    "outputId": "df4ab9ed-52f4-4538-87a5-66932ad5f9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue: $3698.76\n",
      "Expected Revenue: $753.57\n"
     ]
    }
   ],
   "source": [
    "all_product_control = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "all_product_treated = np.random.choice([0,1], NUM_Product, p=[0, 1])\n",
    "all_product_control = torch.from_numpy(all_product_control).to(X_user_train.device).bool()\n",
    "all_product_treated = torch.from_numpy(all_product_treated).to(X_user_train.device).bool()\n",
    "\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_control)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "expected_revenue = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue.item():.2f}\")\n",
    "\n",
    "utilities = model(X_user_test, X_product, price, user_randomization, all_product_treated)\n",
    "probabilities = F.softmax(utilities, dim=1)  # Convert utilities to probabilities\n",
    "\n",
    "# Calculate expected revenue\n",
    "price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)*discount\n",
    "expected_revenue_treated = torch.sum(probabilities * price_with_outside.unsqueeze(0).expand_as(probabilities), dim=0).sum()\n",
    "print(f\"Expected Revenue: ${expected_revenue_treated.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "P7Z_BF2C1kdj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7Z_BF2C1kdj",
    "outputId": "6998bb85-85b3-4cf8-da34-4ecded076114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (Treated - Control) by Linear MNL: $-5890.39\n",
      "Absolute Percentage Estimation Error of Linear MNL:  -0.71%\n"
     ]
    }
   ],
   "source": [
    "linear = (expected_revenue_treated-expected_revenue).cpu().detach().numpy()\n",
    "linear = linear*2\n",
    "print(f\"Revenue Difference (Treated - Control) by Linear MNL: ${linear:.2f}\")\n",
    "print(f\"Absolute Percentage Estimation Error of Linear MNL:  {100*np.abs(linear-revenue_difference)/revenue_difference:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309183f7",
   "metadata": {},
   "source": [
    "# Use NMNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c36b1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "707121c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25b8e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearNestedMNL(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(LinearNestedMNL, self).__init__()\n",
    "        \n",
    "  \n",
    "        total_feature_dim = user_feature_dim + product_feature_dim + 1 \n",
    "        self.utility_linear = nn.Linear(total_feature_dim, 1)\n",
    "        self.raw_lambda = nn.Parameter(torch.tensor(2.0)) \n",
    "\n",
    "    def forward(self, x_user, x_product, prices):\n",
    " \n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "\n",
    "        prices_expanded = prices.view(1, M, 1).expand(N, -1, -1)\n",
    "        \n",
    "      \n",
    "        combined_features = torch.cat((\n",
    "            x_user.unsqueeze(1).expand(-1, M, -1),       # (N, M, U_dim)\n",
    "            x_product.unsqueeze(0).expand(N, -1, -1),    # (N, M, P_dim)\n",
    "            prices_expanded                              # (N, M, 1)\n",
    "        ), dim=2)\n",
    "\n",
    "\n",
    "        utilities = self.utility_linear(combined_features).squeeze(-1)\n",
    "\n",
    "       \n",
    "        lam = torch.sigmoid(self.raw_lambda) \n",
    "\n",
    "\n",
    "        scaled_utilities = utilities / lam\n",
    "        inclusive_value_log = torch.logsumexp(scaled_utilities, dim=1, keepdim=True) \n",
    "        \n",
    "        v_buy = lam * inclusive_value_log\n",
    "        v_outside = torch.zeros(N, 1, device=x_user.device)\n",
    "        \n",
    "        # Log Softmax over the two Nests\n",
    "        nest_logits = torch.cat([v_buy, v_outside], dim=1)\n",
    "        nest_log_probs = F.log_softmax(nest_logits, dim=1) \n",
    "        \n",
    "        log_prob_buy_nest = nest_log_probs[:, 0].unsqueeze(1)\n",
    "        log_prob_outside = nest_log_probs[:, 1].unsqueeze(1)\n",
    "\n",
    "        log_prob_item_given_buy = scaled_utilities - inclusive_value_log\n",
    "        final_log_probs_products = log_prob_item_given_buy + log_prob_buy_nest\n",
    "        \n",
    "        # Return: [Log P(Outside), Log P(Prod 1), ..., Log P(Prod M)]\n",
    "        return torch.cat([log_prob_outside, final_log_probs_products], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44ae9fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a4a9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 2.4466, Validation Loss: 2.4397\n",
      "Epoch 100, Training Loss: 2.3047, Validation Loss: 2.3172\n",
      "Epoch 200, Training Loss: 2.2930, Validation Loss: 2.3061\n",
      "Epoch 300, Training Loss: 2.2880, Validation Loss: 2.3010\n",
      "Epoch 400, Training Loss: 2.2855, Validation Loss: 2.2981\n",
      "Epoch 500, Training Loss: 2.2841, Validation Loss: 2.2964\n",
      "Epoch 600, Training Loss: 2.2832, Validation Loss: 2.2953\n",
      "Epoch 700, Training Loss: 2.2827, Validation Loss: 2.2947\n",
      "Epoch 800, Training Loss: 2.2823, Validation Loss: 2.2942\n",
      "Epoch 900, Training Loss: 2.2821, Validation Loss: 2.2939\n",
      "Final Lambda (Nesting Parameter): 0.9631\n",
      "Expected Revenue (Control): $3689.40\n",
      "Expected Revenue (Treated): $751.39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LinearNestedMNL(\n",
    "    user_feature_dim=X_user_train1.shape[1], \n",
    "    product_feature_dim=X_product.shape[1]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()  \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    log_probs = model(user_features, product_features, prices)\n",
    "    loss = -torch.mean(log_probs[torch.arange(log_probs.shape[0]), decision_train1 + 1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_log_probs = model(X_user_val, product_features, prices)\n",
    "        val_loss = -torch.mean(val_log_probs[torch.arange(val_log_probs.shape[0]), decision_val + 1])\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0 \n",
    "    else:\n",
    "        patience_counter += 1 \n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at Epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Lambda (Nesting Parameter): {torch.sigmoid(model.raw_lambda).item():.4f}\")\n",
    "\n",
    "\n",
    "def calculate_expected_revenue(model, user_features, product_features, prices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        log_probs = model(user_features, product_features, prices)\n",
    "        probabilities = torch.exp(log_probs)\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device), prices), dim=0)\n",
    "        \n",
    "        total_expected_revenue = (probabilities * price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "    return total_expected_revenue.item()\n",
    "\n",
    "\n",
    "X_user_test = X_user_test.to(device)\n",
    "X_product = X_product.to(device)\n",
    "price = price.to(device)\n",
    "\n",
    "\n",
    "user_features_test, product_features_test, prices_control, _ = prepare_data(X_user_test, X_product, price)\n",
    "\n",
    "expected_revenue_control = calculate_expected_revenue(\n",
    "    model, user_features_test, product_features_test, prices_control\n",
    ")\n",
    "print(f\"Expected Revenue (Control): ${expected_revenue_control:.2f}\")\n",
    "\n",
    "all_treated_price = price * discount\n",
    "user_features_test, product_features_test, prices_treated, _ = prepare_data(X_user_test, X_product, all_treated_price)\n",
    "\n",
    "expected_revenue_treated = calculate_expected_revenue(\n",
    "    model, user_features_test, product_features_test, prices_treated\n",
    ")\n",
    "print(f\"Expected Revenue (Treated): ${expected_revenue_treated:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "252c6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference: $-5876.02\n",
      "Absolute Percentage Estimation Error of PDL:  -0.47%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Revenue Difference: ${2*(expected_revenue_treated - expected_revenue_control):.2f}\")\n",
    "nmnl = 2*(expected_revenue_treated - expected_revenue_control)\n",
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(nmnl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd460",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# use PDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e9445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab4aae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2899bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "134acfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39864359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(PDLModel, self).__init__()\n",
    "        # Combined feature dimension includes product features, price, and user features, as well as other products' features and prices\n",
    "        total_feature_dim = user_feature_dim + 2*product_feature_dim + 1  # +1 for price\n",
    "\n",
    "        # Single neural network to process the combined features\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1) \n",
    "        )\n",
    "            # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_user, x_product, x_other_products,prices):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                        x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        aggregated_other_features.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        prices.view(1, -1, 1).expand(N, -1, -1)),\n",
    "                                        dim=2)\n",
    "   \n",
    "\n",
    "        # Compute utility for each combined feature set\n",
    "        utilities = self.network(combined_features).squeeze(-1)\n",
    "\n",
    "        # Incorporate the outside option with utility 0\n",
    "        zero_utilities = torch.zeros(N, 1, device=utilities.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utilities), dim=1)\n",
    "\n",
    "        return utilities_with_outside\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a160035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdlmodel = PDLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1ceb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.437103033065796, Validation Loss: 2.433465003967285\n",
      "Epoch 2, Training Loss: 2.4329981803894043, Validation Loss: 2.429652452468872\n",
      "Epoch 3, Training Loss: 2.428769111633301, Validation Loss: 2.4249675273895264\n",
      "Epoch 4, Training Loss: 2.4241280555725098, Validation Loss: 2.42000412940979\n",
      "Epoch 5, Training Loss: 2.4191954135894775, Validation Loss: 2.4132778644561768\n",
      "Epoch 6, Training Loss: 2.411602020263672, Validation Loss: 2.403303384780884\n",
      "Epoch 7, Training Loss: 2.4013025760650635, Validation Loss: 2.3920788764953613\n",
      "Epoch 8, Training Loss: 2.389850616455078, Validation Loss: 2.3795416355133057\n",
      "Epoch 9, Training Loss: 2.377100706100464, Validation Loss: 2.3660919666290283\n",
      "Epoch 10, Training Loss: 2.3633103370666504, Validation Loss: 2.3523190021514893\n",
      "Epoch 11, Training Loss: 2.3489365577697754, Validation Loss: 2.3389687538146973\n",
      "Epoch 12, Training Loss: 2.3346118927001953, Validation Loss: 2.3269262313842773\n",
      "Epoch 13, Training Loss: 2.3211324214935303, Validation Loss: 2.317143678665161\n",
      "Epoch 14, Training Loss: 2.3093442916870117, Validation Loss: 2.310464859008789\n",
      "Epoch 15, Training Loss: 2.29997181892395, Validation Loss: 2.30743408203125\n",
      "Epoch 16, Training Loss: 2.293501377105713, Validation Loss: 2.3078510761260986\n",
      "Epoch 17, Training Loss: 2.290052652359009, Validation Loss: 2.31048846244812\n",
      "Epoch 18, Training Loss: 2.2890799045562744, Validation Loss: 2.3135673999786377\n",
      "Epoch 19, Training Loss: 2.2894392013549805, Validation Loss: 2.3150646686553955\n",
      "Epoch 20, Training Loss: 2.2897117137908936, Validation Loss: 2.313901901245117\n",
      "Epoch 21, Training Loss: 2.2890543937683105, Validation Loss: 2.310335159301758\n",
      "Epoch 22, Training Loss: 2.287616491317749, Validation Loss: 2.305595636367798\n",
      "Epoch 23, Training Loss: 2.286250352859497, Validation Loss: 2.3011858463287354\n",
      "Epoch 24, Training Loss: 2.2857837677001953, Validation Loss: 2.297985792160034\n",
      "Epoch 25, Training Loss: 2.28609561920166, Validation Loss: 2.2958056926727295\n",
      "Epoch 26, Training Loss: 2.286062479019165, Validation Loss: 2.2942774295806885\n",
      "Epoch 27, Training Loss: 2.2850663661956787, Validation Loss: 2.2935924530029297\n",
      "Epoch 28, Training Loss: 2.2836058139801025, Validation Loss: 2.294116258621216\n",
      "Epoch 29, Training Loss: 2.282571315765381, Validation Loss: 2.295701742172241\n",
      "Epoch 30, Training Loss: 2.282443046569824, Validation Loss: 2.297407388687134\n",
      "Epoch 31, Training Loss: 2.282928705215454, Validation Loss: 2.2980828285217285\n",
      "Epoch 32, Training Loss: 2.283325433731079, Validation Loss: 2.2972609996795654\n",
      "Epoch 33, Training Loss: 2.283247470855713, Validation Loss: 2.2953226566314697\n",
      "Epoch 34, Training Loss: 2.2828562259674072, Validation Loss: 2.2930591106414795\n",
      "Epoch 35, Training Loss: 2.282554864883423, Validation Loss: 2.29118275642395\n",
      "Epoch 36, Training Loss: 2.282590389251709, Validation Loss: 2.290041446685791\n",
      "Epoch 37, Training Loss: 2.2828571796417236, Validation Loss: 2.2896275520324707\n",
      "Epoch 38, Training Loss: 2.28303861618042, Validation Loss: 2.2898073196411133\n",
      "Epoch 39, Training Loss: 2.282926559448242, Validation Loss: 2.2904903888702393\n",
      "Epoch 40, Training Loss: 2.2825777530670166, Validation Loss: 2.291600227355957\n",
      "Epoch 41, Training Loss: 2.2822036743164062, Validation Loss: 2.292954921722412\n",
      "Epoch 42, Training Loss: 2.2819736003875732, Validation Loss: 2.2942237854003906\n",
      "Epoch 43, Training Loss: 2.28190016746521, Validation Loss: 2.2950401306152344\n",
      "Epoch 44, Training Loss: 2.2818710803985596, Validation Loss: 2.295192003250122\n",
      "Epoch 45, Training Loss: 2.2817819118499756, Validation Loss: 2.294731616973877\n",
      "Epoch 46, Training Loss: 2.281630277633667, Validation Loss: 2.2939114570617676\n",
      "Epoch 47, Training Loss: 2.281493902206421, Validation Loss: 2.2930305004119873\n",
      "Epoch 48, Training Loss: 2.2814419269561768, Validation Loss: 2.292309284210205\n",
      "Epoch 49, Training Loss: 2.281472682952881, Validation Loss: 2.291851758956909\n",
      "Epoch 50, Training Loss: 2.281525135040283, Validation Loss: 2.291673183441162\n",
      "Epoch 51, Training Loss: 2.2815356254577637, Validation Loss: 2.291748046875\n",
      "Epoch 52, Training Loss: 2.2814829349517822, Validation Loss: 2.2920312881469727\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(pdlmodel.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pdlmodel.train()  \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = pdlmodel(user_features, product_features, all_x_other_products,prices)\n",
    "    choice_probabilities = F.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]),decision_train1+1])\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    pdlmodel.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_outputs = pdlmodel(X_user_val,  product_features, all_x_other_products,prices)\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        # torch.save(pdlmodel.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b5374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c915a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3654.28\n",
      "Expected Revenue all treated: $738.34\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(pdlmodel, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17cb1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl = (expected_revenue_all_treated-expected_revenue_all_control)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a4df632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of PDL:  -0.29%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(pdl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c900cf",
   "metadata": {
    "id": "63c900cf"
   },
   "source": [
    "# use dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecd9bca0-6191-4297-8452-7f8af22d5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityEstimator(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(UtilityEstimator, self).__init__()\n",
    "        \n",
    "        # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product-1), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, product_feature_dim)\n",
    "        )\n",
    "\n",
    "        self.theta0 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        # Output layer for Theta1 (takes xi, zj, z-j, p-j)\n",
    "        self.theta1 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + 2 * product_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_user, x_product, x_other_products,price):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "    \n",
    "\n",
    "        # Combine features for Theta0\n",
    "        \n",
    "        combined_features_theta =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                               x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                               aggregated_other_features.unsqueeze(0).expand(N, -1, -1)),\n",
    "                                                 dim=2)\n",
    "        theta0_output = self.theta0(combined_features_theta).squeeze(-1)\n",
    "        theta1_output = self.theta1(combined_features_theta).squeeze(-1)\n",
    "        \n",
    "        price = price.unsqueeze(-1)  \n",
    "        utility = theta0_output + theta1_output * price.squeeze(-1)\n",
    "\n",
    "        # Include the outside option (utility = 0)\n",
    "        zero_utilities = torch.zeros(x_user.shape[0], 1, device=utility.device)\n",
    "        utilities_with_outside = torch.cat((zero_utilities, utility), dim=1)\n",
    "        \n",
    "        return utilities_with_outside,theta0_output,theta1_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf77eeb6",
   "metadata": {
    "id": "cf77eeb6"
   },
   "outputs": [],
   "source": [
    "dml_model = UtilityEstimator(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "QT_wrrh3rIws",
   "metadata": {
    "id": "QT_wrrh3rIws"
   },
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31b522ed-0c36-4199-a309-74f71aece365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_features, product_features, prices):\n",
    "    num_products = product_features.shape[0]\n",
    "    all_x_other_products = []\n",
    "    for i in range(num_products):\n",
    "        indices = [j for j in range(num_products) if j != i]\n",
    "        other_products = product_features[indices].reshape(-1)\n",
    "        all_x_other_products.append(other_products)\n",
    "\n",
    "    # Convert lists to tensor\n",
    "    all_x_other_products = torch.stack(all_x_other_products, dim=0)\n",
    "  \n",
    "\n",
    "    return user_features, product_features, prices, all_x_other_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0ff32e5-5c64-49ad-bbf0-f6aa2a53d0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4500, 5]),\n",
       " torch.Size([10, 5]),\n",
       " torch.Size([10]),\n",
       " torch.Size([10, 45]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = price.to(device)\n",
    "prepared_data = prepare_data(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = prepared_data\n",
    "user_features.shape, product_features.shape, prices.shape, all_x_other_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b386e12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b386e12",
    "outputId": "aab71afc-5217-4c73-eba6-26cb31bed335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.433765172958374, Validation Loss: 2.4297828674316406\n",
      "Epoch 2, Training Loss: 2.426105499267578, Validation Loss: 2.422823905944824\n",
      "Epoch 3, Training Loss: 2.419569969177246, Validation Loss: 2.4168713092803955\n",
      "Epoch 4, Training Loss: 2.4141645431518555, Validation Loss: 2.411648988723755\n",
      "Epoch 5, Training Loss: 2.4094736576080322, Validation Loss: 2.4070522785186768\n",
      "Epoch 6, Training Loss: 2.405425786972046, Validation Loss: 2.403066873550415\n",
      "Epoch 7, Training Loss: 2.4020044803619385, Validation Loss: 2.400482654571533\n",
      "Epoch 8, Training Loss: 2.3996801376342773, Validation Loss: 2.399214744567871\n",
      "Epoch 9, Training Loss: 2.3990750312805176, Validation Loss: 2.3985023498535156\n",
      "Epoch 10, Training Loss: 2.398787260055542, Validation Loss: 2.3976340293884277\n",
      "Epoch 11, Training Loss: 2.3979554176330566, Validation Loss: 2.3964719772338867\n",
      "Epoch 12, Training Loss: 2.3966023921966553, Validation Loss: 2.3952043056488037\n",
      "Epoch 13, Training Loss: 2.3950040340423584, Validation Loss: 2.3940539360046387\n",
      "Epoch 14, Training Loss: 2.3934326171875, Validation Loss: 2.3930511474609375\n",
      "Epoch 15, Training Loss: 2.3920581340789795, Validation Loss: 2.392150402069092\n",
      "Epoch 16, Training Loss: 2.3908820152282715, Validation Loss: 2.3913445472717285\n",
      "Epoch 17, Training Loss: 2.3898143768310547, Validation Loss: 2.390510320663452\n",
      "Epoch 18, Training Loss: 2.3887598514556885, Validation Loss: 2.3897485733032227\n",
      "Epoch 19, Training Loss: 2.3875486850738525, Validation Loss: 2.388599157333374\n",
      "Epoch 20, Training Loss: 2.385897159576416, Validation Loss: 2.3866825103759766\n",
      "Epoch 21, Training Loss: 2.383260488510132, Validation Loss: 2.383744239807129\n",
      "Epoch 22, Training Loss: 2.379591941833496, Validation Loss: 2.38010835647583\n",
      "Epoch 23, Training Loss: 2.374964475631714, Validation Loss: 2.37522029876709\n",
      "Epoch 24, Training Loss: 2.3695313930511475, Validation Loss: 2.3696651458740234\n",
      "Epoch 25, Training Loss: 2.3635950088500977, Validation Loss: 2.3637707233428955\n",
      "Epoch 26, Training Loss: 2.3572700023651123, Validation Loss: 2.3576292991638184\n",
      "Epoch 27, Training Loss: 2.35070538520813, Validation Loss: 2.3515217304229736\n",
      "Epoch 28, Training Loss: 2.3437483310699463, Validation Loss: 2.345069408416748\n",
      "Epoch 29, Training Loss: 2.3364007472991943, Validation Loss: 2.3386409282684326\n",
      "Epoch 30, Training Loss: 2.3286375999450684, Validation Loss: 2.332477569580078\n",
      "Epoch 31, Training Loss: 2.3206725120544434, Validation Loss: 2.3270561695098877\n",
      "Epoch 32, Training Loss: 2.313066005706787, Validation Loss: 2.32303786277771\n",
      "Epoch 33, Training Loss: 2.306575059890747, Validation Loss: 2.32084321975708\n",
      "Epoch 34, Training Loss: 2.3018906116485596, Validation Loss: 2.3200879096984863\n",
      "Epoch 35, Training Loss: 2.299095869064331, Validation Loss: 2.319591522216797\n",
      "Epoch 36, Training Loss: 2.2973685264587402, Validation Loss: 2.3177735805511475\n",
      "Epoch 37, Training Loss: 2.2954866886138916, Validation Loss: 2.313835620880127\n",
      "Epoch 38, Training Loss: 2.292729616165161, Validation Loss: 2.3084166049957275\n",
      "Epoch 39, Training Loss: 2.2896206378936768, Validation Loss: 2.3031005859375\n",
      "Epoch 40, Training Loss: 2.2873852252960205, Validation Loss: 2.2990598678588867\n",
      "Epoch 41, Training Loss: 2.286885976791382, Validation Loss: 2.296491861343384\n",
      "Epoch 42, Training Loss: 2.2873730659484863, Validation Loss: 2.2945401668548584\n",
      "Epoch 43, Training Loss: 2.287187099456787, Validation Loss: 2.292569637298584\n",
      "Epoch 44, Training Loss: 2.2860403060913086, Validation Loss: 2.2914106845855713\n",
      "Epoch 45, Training Loss: 2.284512519836426, Validation Loss: 2.2908756732940674\n",
      "Epoch 46, Training Loss: 2.2832064628601074, Validation Loss: 2.2913434505462646\n",
      "Epoch 47, Training Loss: 2.28253436088562, Validation Loss: 2.292391538619995\n",
      "Epoch 48, Training Loss: 2.2820327281951904, Validation Loss: 2.2934694290161133\n",
      "Epoch 49, Training Loss: 2.281795024871826, Validation Loss: 2.2943360805511475\n",
      "Epoch 50, Training Loss: 2.2816600799560547, Validation Loss: 2.2947983741760254\n",
      "Epoch 51, Training Loss: 2.281569480895996, Validation Loss: 2.2948598861694336\n",
      "Epoch 52, Training Loss: 2.2815403938293457, Validation Loss: 2.294595241546631\n",
      "Epoch 53, Training Loss: 2.2815804481506348, Validation Loss: 2.294116973876953\n",
      "Epoch 54, Training Loss: 2.281766653060913, Validation Loss: 2.2936182022094727\n",
      "Epoch 55, Training Loss: 2.2820191383361816, Validation Loss: 2.293116807937622\n",
      "Epoch 56, Training Loss: 2.28214955329895, Validation Loss: 2.2926158905029297\n",
      "Epoch 57, Training Loss: 2.282041311264038, Validation Loss: 2.2921953201293945\n",
      "Epoch 58, Training Loss: 2.281730890274048, Validation Loss: 2.2920162677764893\n",
      "Epoch 59, Training Loss: 2.281358003616333, Validation Loss: 2.291945695877075\n",
      "Epoch 60, Training Loss: 2.2810866832733154, Validation Loss: 2.2921223640441895\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "optimizer = torch.optim.Adam(dml_model.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dml_model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = dml_model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1+1 ])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    dml_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = dml_model(X_user_val,  product_features, all_x_other_products,prices)[0]\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val+1])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "        torch.save(dml_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b27e55e",
   "metadata": {
    "id": "1b27e55e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_expected_revenue(model,user_features, product_features, all_x_other_products,prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        utilities = model(user_features, product_features, all_x_other_products,prices)[0]\n",
    "        probabilities = F.softmax(utilities, dim=1)  # Softmax over products only\n",
    "\n",
    "        # Calculate expected revenue for each product\n",
    "        price_with_outside = torch.cat((torch.zeros(1, device=prices.device),prices), dim=0)\n",
    "        total_expected_revenue = (probabilities.sum(dim=0)* price_with_outside.unsqueeze(0)).sum()\n",
    "\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5efd256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $3662.25\n",
      "Expected Revenue all treated: $739.45\n"
     ]
    }
   ],
   "source": [
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, all_x_other_products = control_prepared_data\n",
    "# Calculate expected revenue\n",
    "expected_revenue_all_control = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, all_x_other_products = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(dml_model, user_features, product_features, all_x_other_products, prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "QGABODM51OV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGABODM51OV4",
    "outputId": "3769a33f-2282-4787-e276-3f7d3b56c540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2922.802490234375"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121cd47",
   "metadata": {
    "id": "1121cd47"
   },
   "source": [
    "# debias the GTE estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "SMTzkngzuUls",
   "metadata": {
    "id": "SMTzkngzuUls"
   },
   "outputs": [],
   "source": [
    "test_prepared_data = prepare_data(X_user_test, X_product,  price*(discount*prod_randomization))\n",
    "user_features, product_features, prices, all_x_other_products = test_prepared_data\n",
    "\n",
    "# Compute Theta0 and Theta1\n",
    "_,theta0_output,theta1_output = dml_model(user_features, product_features, all_x_other_products,prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de3fecfc-2f03-48e9-a089-5cebf4ac0f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 10])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rag3u55FWjiu",
   "metadata": {
    "id": "Rag3u55FWjiu"
   },
   "source": [
    "# use formulation debias for H_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "SSXrdFP4WnZL",
   "metadata": {
    "id": "SSXrdFP4WnZL"
   },
   "outputs": [],
   "source": [
    "def H_theta(theta0_output,theta1_output,all_treated_price,price):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = all_treated_price.unsqueeze(0).expand(N, M)\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_treated_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    all_treated_probabilities = F.softmax(all_treated_uti, dim=1)\n",
    "    all_control_probabilities = F.softmax(all_control_uti, dim=1)\n",
    "\n",
    "    price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "    treated_price_with_outside =  torch.cat((torch.zeros(1, device=all_treated_price.device),all_treated_price), dim=0)\n",
    "\n",
    "    H = torch.sum(all_treated_probabilities*treated_price_with_outside - all_control_probabilities*price_with_outside,dim=1)\n",
    "    expsum_treated = torch.sum(torch.exp(all_treated_uti),dim=1)\n",
    "    expsum_control = torch.sum(torch.exp(all_control_uti),dim=1)\n",
    "\n",
    "    expsum_treated_expanded = expsum_treated.unsqueeze(1).expand(-1, all_treated_uti.shape[1])  # Shape [N, M+1]\n",
    "    expsum_control_expanded = expsum_control.unsqueeze(1).expand(-1, all_control_uti.shape[1])  # Shape [N, M+1]\n",
    "\n",
    "    H_theta0 = torch.sum((torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded-\\\n",
    "                          torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded)\\\n",
    "                         *price_with_outside,dim=1)\n",
    "    H_theta1 = torch.sum(price_with_outside*(torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded*treated_price_with_outside-\\\n",
    "                                             torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded*price_with_outside),dim=1)\n",
    "\n",
    "\n",
    "    return H,H_theta0,H_theta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "BFHZQM3VfBUI",
   "metadata": {
    "id": "BFHZQM3VfBUI"
   },
   "outputs": [],
   "source": [
    "H,H_theta0,H_theta1 = H_theta(theta0_output,theta1_output,all_treated_price,price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "yivIC_MKad5x",
   "metadata": {
    "id": "yivIC_MKad5x"
   },
   "outputs": [],
   "source": [
    "def l_theta(theta0_output,theta1_output,adjusted_price,decision_test):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_adjusted_price = adjusted_price.unsqueeze(0).expand(N, M)\n",
    "    uti = theta0_output + theta1_output * expand_adjusted_price\n",
    "    adjusted_price_with_outside =  torch.cat([torch.zeros(1, device=adjusted_price.device),adjusted_price])\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=uti.device)\n",
    "    uti = torch.cat((zero_utilities,uti), dim=1)\n",
    "\n",
    "    probabilities = F.softmax(uti, dim=1)\n",
    "    prod_indices = torch.ones(NUM_Product, device=device)\n",
    "    prod_indices = torch.cat([torch.zeros(1,device=device),prod_indices])\n",
    "    ltheta0 = probabilities[torch.arange(decision_test.size(0)), decision_test+1] -prod_indices[decision_test+1]\n",
    "    ltheta1 = (probabilities[torch.arange(decision_test.size(0)), decision_test+1] * adjusted_price_with_outside[decision_test+1]) - adjusted_price_with_outside[decision_test+1]\n",
    "\n",
    "\n",
    "    return ltheta0,ltheta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77bcb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "O8c-tupIgHVu",
   "metadata": {
    "id": "O8c-tupIgHVu"
   },
   "outputs": [],
   "source": [
    "adjusted_price = price*(discount*prod_randomization).to(device)\n",
    "decision_test = decision_test.to(device)\n",
    "ltheta0,ltheta1= l_theta(theta0_output,theta1_output,adjusted_price,decision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "UVRht78QaxSG",
   "metadata": {
    "id": "UVRht78QaxSG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lambdainv(theta0_output, theta1_output, price, decision_test,epsilon =10):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = discount*price.unsqueeze(0).expand(N, M)\n",
    "\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_control_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    # Calculate probabilities using softmax\n",
    "    probabilities_control = F.softmax(all_control_uti, dim=1)\n",
    "    probabilities_treated = F.softmax(all_treated_uti, dim=1)\n",
    "\n",
    "    # Extract probabilities of chosen products\n",
    "    chosen_prob_control = probabilities_control[torch.arange(N), decision_test]\n",
    "    chosen_prob_treated = probabilities_treated[torch.arange(N), decision_test]\n",
    "\n",
    "    # Calculate second derivatives\n",
    "    ltheta00 = -chosen_prob_control * (1 - chosen_prob_control) - chosen_prob_treated * (1 - chosen_prob_treated)\n",
    "    ltheta01 = -chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test] - \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])\n",
    "    ltheta11 = -chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test]**2 - \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])**2\n",
    "    ltheta00=ltheta00/2\n",
    "    ltheta01=ltheta01/2\n",
    "    ltheta11=ltheta11/2\n",
    "\n",
    "    # Form the 2x2 Hessian matrices for each instance\n",
    "    ltheta00 = ltheta00.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta01 = ltheta01.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta11 = ltheta11.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    top_row = torch.cat((ltheta00, ltheta01), dim=2)\n",
    "    bottom_row = torch.cat((ltheta01, ltheta11), dim=2)\n",
    "\n",
    "    L_matrix = torch.cat((top_row, bottom_row), dim=1)\n",
    "\n",
    "    # Regularization and inversion\n",
    "    \n",
    "    identity_matrix = torch.eye(2, dtype=L_matrix.dtype, device=L_matrix.device) * epsilon\n",
    "    L_matrix_reg = L_matrix + identity_matrix.unsqueeze(0).unsqueeze(0)\n",
    "    L_inv = torch.linalg.inv(L_matrix_reg)\n",
    "\n",
    "    return L_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6f7441c-eea7-4f98-8631-71cb75d14600",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0.001,0.01,0.1,0.5,1,5,10]\n",
    "min_mape = float('inf')\n",
    "best_epsilon = None\n",
    "best_final_result = None\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    # Update L_inv for the current epsilon\n",
    "    try:\n",
    "        L_inv = lambdainv(theta0_output, theta1_output, price, decision_test, epsilon).float()\n",
    "    \n",
    "        # Calculate final_result with the given epsilon\n",
    "        H_theta_array = torch.stack((H_theta0, H_theta1), dim=-1).unsqueeze(1).float()  \n",
    "        l_theta_array = torch.stack((ltheta0, ltheta1), dim=-1).unsqueeze(-1).float()  \n",
    "    \n",
    "        # Perform matrix multiplications\n",
    "        result_intermediate = torch.matmul(H_theta_array, L_inv.squeeze(0)) \n",
    "        final_result = torch.matmul(result_intermediate, l_theta_array).squeeze(-1)  \n",
    "        final_result[torch.isnan(final_result) | torch.isinf(final_result)] = 0\n",
    "    \n",
    "        # Calculate sdl and dedl\n",
    "        sdl = H.sum().cpu().detach().numpy() * 2\n",
    "        dedl = (H.sum().cpu().detach().numpy() - final_result.sum().cpu().detach().numpy()) * 2\n",
    "    \n",
    "        # Calculate MAPE of dedl with respect to true\n",
    "        mape_dedl = np.abs((dedl - true) / true)\n",
    "    \n",
    "        # Update best_epsilon if the current epsilon yields a lower MAPE\n",
    "        if mape_dedl < min_mape:\n",
    "            min_mape = mape_dedl\n",
    "            best_epsilon = epsilon\n",
    "            best_final_result = final_result\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "q11HQu-goWM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q11HQu-goWM0",
    "outputId": "ecf71629-8dbc-4d80-8d4a-de1f2534e3eb"
   },
   "outputs": [],
   "source": [
    "sdl = H.sum().cpu().detach().numpy()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3a445f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedl = (H.sum().cpu().detach().numpy()-best_final_result.sum().cpu().detach().numpy())*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c6d8d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5845.60498046875, -5843.46044921875, 10)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl,dedl,best_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d44fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of SDL:  -0.05%\n",
      "Absolute Percentage Estimation Error of SP MNL:  -0.09%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of SDL:  {100*np.abs(sdl-revenue_difference)/revenue_difference:.2f}%\")\n",
    "print(f\"Absolute Percentage Estimation Error of SP MNL:  {100*np.abs(dedl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eeea3d7d-d20e-48c8-b037-107fce171386",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_pe = (naive - true) / true\n",
    "linear_pe = (linear - true) / true\n",
    "pdl_pe = (pdl - true) / true\n",
    "sdl_pe = (sdl - true) / true\n",
    "dedl_pe = (dedl - true) / true\n",
    "naive_mse = (naive - true)**2\n",
    "linear_mse =(linear - true)**2\n",
    "pdl_mse = (pdl - true)**2\n",
    "sdl_mse = (sdl - true)**2\n",
    "dedl_mse = (dedl - true)**2\n",
    "naive_e = (naive - true)\n",
    "linear_e =(linear - true)\n",
    "pdl_e = (pdl - true)\n",
    "sdl_e = (sdl - true)\n",
    "dedl_e = (dedl - true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1094fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07819295965840456 0.007142810067716681 -0.002862608460443805 -0.0005143370783542302 -0.0008810105441843756 -457.3203712403774 -41.77553281188011 16.74228945374489 3.0081585943698883 5.152689844369888 209141.92195143664 1745.1951417164719 280.3042561529773 9.049018128881423 26.550212632272583\n"
     ]
    }
   ],
   "source": [
    "print(naive_pe,linear_pe,pdl_pe,sdl_pe,dedl_pe,naive_e,linear_e,pdl_e,sdl_e,dedl_e,naive_mse,linear_mse,pdl_mse,sdl_mse,dedl_mse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "c2a5b9ca",
    "7e135e9e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
