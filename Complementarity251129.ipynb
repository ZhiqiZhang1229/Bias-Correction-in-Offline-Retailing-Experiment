{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27192f",
   "metadata": {
    "id": "6c27192f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14efbb6",
   "metadata": {
    "id": "f14efbb6"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56819b49-b3bb-4d32-ad88-a458e7ee142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ff42b4-0dd7-4555-bfbc-c1d6a7c65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_Product = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42efd20-8c8f-4675-aa1a-9a2534ee60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_percentage = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce9a174-68e6-47f3-ad2f-966cc2698e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc24047e-fa23-40ae-b761-5f0d1fbe595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338e9f4b-a5a7-47a9-b06d-5216d077594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_continuous_feature_multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9083308b",
   "metadata": {
    "id": "9083308b"
   },
   "outputs": [],
   "source": [
    "# Set constants\n",
    "USER_Cont_FEATURES = 2*user_continuous_feature_multiplier\n",
    "USER_Dicr_FEATURES = 3\n",
    "\n",
    "Product_Cont_FEATURES = 3*prod_continuous_feature_multiplier\n",
    "Product_Dicr_FEATURES = 2\n",
    "OUTSIDE_OPTION_UTILITY = 0\n",
    "utilities = torch.zeros(NUM_USER, NUM_Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442d9dd4",
   "metadata": {
    "id": "442d9dd4"
   },
   "outputs": [],
   "source": [
    "def generate_features(N, C, D):\n",
    "    continuous_features = np.zeros((N, C))\n",
    "    for i in range(C):\n",
    "        continuous_features[:, i] = np.random.uniform(0,1,size=N)\n",
    "    binary_features = np.random.randint(0, 2, (N, D))\n",
    "    return np.hstack((continuous_features, binary_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb84b940",
   "metadata": {
    "id": "cb84b940"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class UtilityDNN(nn.Module):\n",
    "    def __init__(self, user_features, product_features):\n",
    "        super(UtilityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features + product_features, 1)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.uniform_(self.fc1.weight, a=-0.0, b=0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class PriceSensitivityDNN(nn.Module):\n",
    "    def __init__(self, user_features):\n",
    "        super(PriceSensitivityDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(user_features,1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "\n",
    "        nn.init.constant_(self.fc1.weight, 0)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.weight, 0)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.abs(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be800ea9",
   "metadata": {
    "id": "be800ea9"
   },
   "outputs": [],
   "source": [
    "X_user = generate_features(NUM_USER,USER_Cont_FEATURES, USER_Dicr_FEATURES)\n",
    "X_product = generate_features(NUM_Product, Product_Cont_FEATURES, Product_Dicr_FEATURES)\n",
    "price = np.random.uniform(0.5 ,1, NUM_Product)\n",
    "\n",
    "X_user = torch.from_numpy(X_user).float()\n",
    "X_product = torch.from_numpy(X_product).float()\n",
    "price = torch.from_numpy(price).float()\n",
    "gumbel_dist = torch.distributions.Gumbel(0, 1)\n",
    "gumbel_noise = gumbel_dist.sample((NUM_USER, NUM_Product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b15fbba-7cea-4bbe-a6b4-5decb6f40ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7871, 0.7414, 0.5153, 0.5254])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b1ab05",
   "metadata": {
    "id": "13b1ab05"
   },
   "outputs": [],
   "source": [
    "pair_utility_model = UtilityDNN(X_user.shape[1], X_product.shape[1])\n",
    "price_sensitivity_model = PriceSensitivityDNN(X_user.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "590d5d95-5c75-43e6-ae03-eddddbfd3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.1368, 0.4839, 0.3859, 0.4845, 0.2574, 0.4539, 0.2923, 0.2170, 0.3591,\n",
      "         0.0578]])\n",
      "Biases: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "layer_weights = pair_utility_model.fc1.weight.data\n",
    "layer_biases = pair_utility_model.fc1.bias.data\n",
    "\n",
    "print(\"Weights:\", layer_weights)\n",
    "print(\"Biases:\", layer_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb32ec0-62e7-4760-8bda-c816621ef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_biases = torch.from_numpy(np.array(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "084c7cb6-2ebc-4cf1-92ea-d229b229bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new weights and biases\n",
    "with torch.no_grad():  # Avoid tracking this operation in the computation graph\n",
    "    price_sensitivity_model.fc2.bias.copy_(new_biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e280f46-89ba-422d-9bd1-6dc50dab0712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.]])\n",
      "Biases: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "layer_weights = price_sensitivity_model.fc2.weight.data\n",
    "layer_biases = price_sensitivity_model.fc2.bias.data\n",
    "\n",
    "print(\"Weights:\", layer_weights)\n",
    "print(\"Biases:\", layer_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d850d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bundles = 2 ** X_product.shape[0]\n",
    "bundle_utilities = torch.randn(num_bundles)\n",
    "def utility_model_batched_with_bundles(x_user, X_product, price, user_randomization, prod_randomization, pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10):\n",
    "    num_users = x_user.shape[0]\n",
    "    num_products = X_product.shape[0]\n",
    "    num_bundles = 2 ** num_products  # Total number of possible bundles\n",
    "    decisions = torch.zeros(num_users, dtype=torch.long)  # Initialize decision array\n",
    "\n",
    "    # Generate all possible bundles (binary representations)\n",
    "    bundle_choices = torch.tensor(\n",
    "    [[int(bit) for bit in np.binary_repr(i, width=num_products)] for i in range(num_bundles)],\n",
    "    dtype=torch.float32)\n",
    "\n",
    "    # Convert numpy arrays to tensors if necessary\n",
    "    if isinstance(user_randomization, np.ndarray):\n",
    "        user_randomization = torch.from_numpy(user_randomization).to(torch.bool)\n",
    "    if isinstance(prod_randomization, np.ndarray):\n",
    "        prod_randomization = torch.from_numpy(prod_randomization).to(torch.bool)\n",
    "    if isinstance(price, np.ndarray):\n",
    "        price = torch.from_numpy(price)\n",
    "\n",
    "    # Compute price sensitivities outside the batch loop\n",
    "    price_sensitivities = price_sensitivity_model(x_user)\n",
    "\n",
    "    # Iterate over users in batches\n",
    "    for i in range(0, num_users, batch_size):\n",
    "        batch_end = min(i + batch_size, num_users)  # Define the end of the batch\n",
    "        batch_indices = slice(i, batch_end)  # Slice for batch indexing\n",
    "\n",
    "        # Repeat the product features and price for each user in the batch\n",
    "        batch_user_features = x_user[batch_indices].unsqueeze(1).expand(-1, num_products, -1)\n",
    "        batch_prod_features = X_product.unsqueeze(0).expand(batch_end - i, -1, -1)\n",
    "        batch_price = price.unsqueeze(0).expand(batch_end - i, -1)\n",
    "\n",
    "        # Handle treatment adjustments in batch\n",
    "        batch_user_treatment = user_randomization[batch_indices].unsqueeze(1).expand(-1, num_products) == 1\n",
    "        batch_prod_treatment = prod_randomization.unsqueeze(0).expand(batch_end - i, -1) == 1\n",
    "        batch_adjusted_price = torch.where(batch_user_treatment | batch_prod_treatment, batch_price * discount, batch_price)\n",
    "\n",
    "        # Combine user and product features\n",
    "        combined_features = torch.cat((batch_user_features, batch_prod_features), dim=2)\n",
    "\n",
    "        # Compute utilities for each user-product pair using the neural network in a batch\n",
    "        utility_from_dnn = pair_utility_model(combined_features.view(-1, combined_features.shape[-1])).view(batch_end - i, num_products)\n",
    "\n",
    "        # Compute price effect\n",
    "        price_effect = price_sensitivities[batch_indices] * batch_adjusted_price\n",
    "        product_utilities = utility_from_dnn - price_effect + gumbel_noise[batch_indices]\n",
    "\n",
    "        # Calculate total bundle utilities by summing product utilities for each bundle and adding bundle-specific utilities\n",
    "        total_bundle_utilities = torch.zeros(batch_end - i, num_bundles)\n",
    "        for b in range(num_bundles):\n",
    "            bundle_mask = bundle_choices[b]  # Binary mask for the current bundle\n",
    "            bundle_utilities_sum = (product_utilities * bundle_mask).sum(dim=1)  # Sum of product utilities in the bundle\n",
    "            total_bundle_utilities[:, b] = bundle_utilities_sum + bundle_utilities[b]  # Add bundle-specific utility\n",
    "\n",
    "        # Find the bundle with the highest utility for each user\n",
    "        max_utilities, chosen_bundles = torch.max(total_bundle_utilities, dim=1)\n",
    "\n",
    "        # The empty bundle (index 0) represents the outside option (no products chosen)\n",
    "        decisions[batch_indices] = torch.where(max_utilities > 0, chosen_bundles, torch.zeros_like(chosen_bundles))\n",
    "\n",
    "    return decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d035359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue_bundle(decisions, prices):\n",
    "    total_revenue = 0.0\n",
    "    num_products = prices.shape[0]\n",
    "\n",
    "    # Iterate over each decision (bundle index) and calculate the total price of the chosen products\n",
    "    for decision in decisions:\n",
    "        if decision != 0:  # Check if the decision is not the outside option (empty bundle)\n",
    "            # Convert the bundle index to a binary mask representing the products in the bundle\n",
    "            bundle_mask = torch.tensor([int(x) for x in np.binary_repr(decision.item(), width=num_products)], dtype=torch.bool)\n",
    "            # Sum the prices of the products included in the bundle\n",
    "            total_revenue += prices[bundle_mask].sum().item()\n",
    "\n",
    "    return total_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97381ec",
   "metadata": {
    "id": "c97381ec"
   },
   "source": [
    "# GTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c9652",
   "metadata": {
    "id": "c74c9652"
   },
   "source": [
    "## All treated scenario: all products are discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8620e2f7",
   "metadata": {
    "id": "8620e2f7"
   },
   "outputs": [],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a69429d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a69429d2",
    "outputId": "da77f021-0faf-424d-91be-088437608bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([15, 14,  7,  ..., 15, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "decisions_all_treat=utility_model_batched_with_bundles(X_user, X_product, price, user_randomization, prod_randomization,\n",
    "                                                         pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10)\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3b7df52-fdc0-46a9-ad64-9119957d6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "all_num_unique = torch.unique(decisions_all_treat).numel()\n",
    "print(all_num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8748518-f99d-47ae-a031-7b89fbbfa669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(7)\n",
      "tensor(207)\n",
      "tensor(11)\n",
      "tensor(13)\n",
      "tensor(0)\n",
      "tensor(744)\n",
      "tensor(188)\n",
      "tensor(66)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(455)\n",
      "tensor(2)\n",
      "tensor(989)\n",
      "tensor(7316)\n"
     ]
    }
   ],
   "source": [
    "for i in range(-1,16):\n",
    "    print(torch.sum(decisions_all_treat==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f59995c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "f59995c9",
    "outputId": "b6b193fc-3ab4-4e76-8096-49dfeae0a46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue from sales when all products are discounted: $4664.38\n"
     ]
    }
   ],
   "source": [
    "total_revenue_all_treated = calculate_revenue_bundle(decisions_all_treat, price*discount)\n",
    "print(f\"Total revenue from sales when all products are discounted: ${total_revenue_all_treated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86abbc5",
   "metadata": {},
   "source": [
    "## All control scenario: all products remain the original price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e8b6df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "34e8b6df",
    "outputId": "846dbf7a-920f-4c54-d79f-1b527ec86ff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions per user (product index or -1 for outside option):\n",
      " tensor([15,  2,  4,  ..., 15, 15, 15])\n",
      "Total Revenue from Sales: $19928.09\n"
     ]
    }
   ],
   "source": [
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, 0])\n",
    "\n",
    "decisions_all_control =utility_model_batched_with_bundles(X_user, X_product, price, user_randomization, prod_randomization,\n",
    "                                                         pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10)\n",
    "\n",
    "print(\"Decisions per user (product index or -1 for outside option):\\n\", decisions_all_control)\n",
    "total_revenue_all_control = calculate_revenue_bundle(decisions_all_control, price)\n",
    "print(f\"Total Revenue from Sales: ${total_revenue_all_control:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e3c0837",
   "metadata": {
    "id": "2e3c0837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (ALLTreated - ALLControl): $-15263.71\n"
     ]
    }
   ],
   "source": [
    "revenue_difference = total_revenue_all_treated - total_revenue_all_control\n",
    "print(f\"Revenue Difference (ALLTreated - ALLControl): ${revenue_difference:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/total_revenue_all_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "Sp6U7JOV1vEi",
   "metadata": {
    "id": "Sp6U7JOV1vEi"
   },
   "outputs": [],
   "source": [
    "true = revenue_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec3bca",
   "metadata": {
    "id": "d0ec3bca"
   },
   "source": [
    "## product randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cbbd718",
   "metadata": {
    "id": "4cbbd718"
   },
   "outputs": [],
   "source": [
    "def calculate_bundle_revenue(decisions, prices, prod_randomization):\n",
    "    revenue_treated = 0.0\n",
    "    revenue_control = 0.0\n",
    "    num_products = prices.shape[0]\n",
    "\n",
    "    # Iterate over each user's decision (bundle index)\n",
    "    for decision in decisions:\n",
    "        if decision != 0:  # If the user did not choose the outside option (empty bundle)\n",
    "            # Convert the bundle index to a binary mask representing the products in the bundle\n",
    "            bundle_mask = torch.tensor([int(x) for x in np.binary_repr(decision.item(), width=num_products)], dtype=torch.bool)\n",
    "\n",
    "            # Iterate over products in the bundle\n",
    "            for product_index in range(num_products):\n",
    "                if bundle_mask[product_index]:  # If the product is included in the bundle\n",
    "                    product_price = prices[product_index].item()\n",
    "                    if prod_randomization[product_index]:  # Check if the product is in the treatment group\n",
    "                        revenue_treated += product_price\n",
    "                    else:  # The product is in the control group\n",
    "                        revenue_control += product_price\n",
    "\n",
    "    return revenue_treated, revenue_control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "402ee3f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "402ee3f8",
    "outputId": "17e270ca-c5e5-4e92-fd2f-2c675eb2fbf8"
   },
   "outputs": [],
   "source": [
    "utilities = torch.zeros(NUM_USER, NUM_Product)\n",
    "user_randomization = np.random.choice([0,1], NUM_USER, p=[1, 0])\n",
    "prod_randomization = np.random.choice([0,1], NUM_Product, p=[1-treatment_percentage, treatment_percentage])\n",
    "# prod_randomization = np.random.choice([0,1], NUM_Product, p=[1, ])\n",
    "decisions_product_randomization =utility_model_batched_with_bundles(X_user, X_product, price, user_randomization, prod_randomization,\n",
    "                                                         pair_utility_model, price_sensitivity_model, gumbel_noise, bundle_utilities, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "022d9af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022d9af1",
    "outputId": "9f338893-e1f9-4c6a-945c-408a2e5b6f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue from Treated Products: $865.61\n",
      "Revenue from Control Products: $16231.20\n",
      "Revenue Difference (Treated - Control) by naive DIM: $-30731.19\n"
     ]
    }
   ],
   "source": [
    "revenue_treated, revenue_control = calculate_bundle_revenue(decisions_product_randomization, price-price*(1-discount)*prod_randomization, prod_randomization)\n",
    "naive = revenue_treated/treatment_percentage - revenue_control/(1-treatment_percentage)\n",
    "print(f\"Revenue from Treated Products: ${revenue_treated:.2f}\")\n",
    "print(f\"Revenue from Control Products: ${revenue_control:.2f}\")\n",
    "print(f\"Revenue Difference (Treated - Control) by naive DIM: ${naive:.2f}\")\n",
    "# print(f\"Revenue Relative Difference (ALLTreated - ALLControl)/AllControl: {100*revenue_difference/revenue_control:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4b9eb",
   "metadata": {},
   "source": [
    "## Prepare training and testing data given experiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "wUgBFRaYHK7-",
   "metadata": {
    "id": "wUgBFRaYHK7-"
   },
   "outputs": [],
   "source": [
    "X_user_1, X_user_2, decision_1, decision_2 = train_test_split(\n",
    "X_user, decisions_product_randomization, test_size=1/2, random_state=3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bd5975d-53cb-45c6-90e2-c36e313515a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\n",
    "    'features': X_user_1,\n",
    "    'labels': decision_1\n",
    "}\n",
    "\n",
    "test_set = {\n",
    "    'features': X_user_2,\n",
    "    'labels': decision_2\n",
    "}\n",
    "\n",
    "# Flag to switch between training and test set\n",
    "use_train_set = False  # Set to False for the test set\n",
    "\n",
    "# Function to get the current active dataset\n",
    "def get_active_dataset(use_train):\n",
    "    return train_set if use_train else test_set\n",
    "def get_test_dataset(use_train):\n",
    "    return test_set if use_train else train_set\n",
    "# Retrieve the current dataset based on the flag\n",
    "current_dataset = get_active_dataset(use_train_set)\n",
    "X_user_train = current_dataset['features']\n",
    "decision_train = current_dataset['labels']\n",
    "X_user_test = get_test_dataset(use_train_set)['features']\n",
    "decision_test =  get_test_dataset(use_train_set)['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16813234",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_choices = torch.tensor(\n",
    "    [[int(bit) for bit in np.binary_repr(i, width=X_product.shape[0])] for i in range(2**X_product.shape[0])],\n",
    "    dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320887a",
   "metadata": {
    "id": "d320887a"
   },
   "source": [
    "# use simple MNL structural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b14cd096",
   "metadata": {
    "id": "b14cd096"
   },
   "outputs": [],
   "source": [
    "class BundleMNLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim, num_bundles):\n",
    "        super(BundleMNLModel, self).__init__()\n",
    "        self.beta_user = nn.Parameter(torch.randn(user_feature_dim))\n",
    "        self.beta_product = nn.Parameter(torch.randn(product_feature_dim))\n",
    "        self.beta_price = nn.Parameter(torch.tensor(-1.0))\n",
    "        self.bundle_utilities = nn.Parameter(torch.randn(num_bundles))  # Bundle-specific utilities\n",
    "\n",
    "    def forward(self, x_user, X_product, price, prod_randomization, bundle_choices):\n",
    "        N, M = x_user.shape[0], X_product.shape[0]\n",
    "        num_bundles = bundle_choices.shape[0]\n",
    "\n",
    "        # Expand user and product features to create a [N, M, F] shaped tensor\n",
    "        x_user_expanded = x_user.unsqueeze(1).expand(-1, M, -1)\n",
    "        X_product_expanded = X_product.unsqueeze(0).expand(N, -1, -1)\n",
    "\n",
    "        # Calculate linear utility from features\n",
    "        utility_user = torch.sum(x_user_expanded * self.beta_user, dim=2)\n",
    "        utility_product = torch.sum(X_product_expanded * self.beta_product, dim=2)\n",
    "\n",
    "        # Adjust prices based on product randomization (apply discount if treated)\n",
    "        adjusted_price = torch.where(prod_randomization.unsqueeze(0), price * discount, price)\n",
    "        utility_price = adjusted_price * self.beta_price\n",
    "\n",
    "        # Total product utilities (user + product + price)\n",
    "        product_utilities = utility_user + utility_product + utility_price\n",
    "\n",
    "        # Calculate total bundle utilities for each user and bundle\n",
    "        total_bundle_utilities = torch.zeros(N, num_bundles, device=x_user.device)\n",
    "        for b in range(num_bundles):\n",
    "            bundle_mask = bundle_choices[b]\n",
    "            bundle_mask = bundle_mask.to(device)\n",
    "            bundle_product_utilities = (product_utilities * bundle_mask).sum(dim=1)\n",
    "            total_bundle_utilities[:, b] = bundle_product_utilities + self.bundle_utilities[b]\n",
    "\n",
    "\n",
    "        return total_bundle_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d027617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89f614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = BundleMNLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES,\n",
    "                       num_bundles = 2 ** NUM_Product).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74e297a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a74e297a",
    "outputId": "d9b4de3b-2c2d-452e-b491-1b217ba2aaf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.7495\n",
      "Epoch 500, Loss: 1.4726\n",
      "Epoch 1000, Loss: 1.4338\n",
      "Epoch 1500, Loss: 1.4303\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import torch.nn.functional as F\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    utilities = model(X_user_train, X_product, price, prod_randomization, bundle_choices)\n",
    "    choice_probabilities = F.log_softmax(utilities, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da3dbeb5-c326-49f2-95d1-948ed3fbb443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.2344,  -7.1493,  -7.1129,  ...,  -7.0896,  -2.7675,  -0.4553],\n",
       "        [-12.5081,  -7.3446,  -7.3082,  ...,  -7.1281,  -2.8061,  -0.4155],\n",
       "        [-10.7963,  -6.1452,  -6.1088,  ...,  -6.9536,  -2.6316,  -0.7534],\n",
       "        ...,\n",
       "        [-10.3872,  -5.8690,  -5.8326,  ...,  -6.9434,  -2.6213,  -0.8761],\n",
       "        [-12.3980,  -7.2659,  -7.2295,  ...,  -7.1123,  -2.7902,  -0.4310],\n",
       "        [ -8.3438,  -4.6073,  -4.5709,  ...,  -7.2449,  -2.9228,  -1.9592]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f78d4b21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f78d4b21",
    "outputId": "6078e9c0-ce87-4f8e-dd3d-88c3cfb2a616"
   },
   "outputs": [],
   "source": [
    "beta_price_est = model.beta_price.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ac97d98-d381-4a84-82c7-aa6821c3aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045324706\n"
     ]
    }
   ],
   "source": [
    "print(beta_price_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98a2386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_prices = torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    bundle_prices[b] = torch.sum(price[bundle_mask.bool()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17eb38d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17eb38d3",
    "outputId": "df4ab9ed-52f4-4538-87a5-66932ad5f9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue (Control Group): $10270.00\n",
      "Expected Revenue (Treated Group): $10213.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Control group (all products are control)\n",
    "all_product_control = torch.zeros(NUM_Product, dtype=torch.bool).to(device)\n",
    "# Treated group (all products are treated)\n",
    "all_product_treated = torch.ones(NUM_Product, dtype=torch.bool).to(device)\n",
    "\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "\n",
    "# Calculate expected revenue for control group\n",
    "utilities_control = model(X_user_test, X_product, price, all_product_control, bundle_choices)\n",
    "probabilities_control = F.softmax(utilities_control, dim=1)\n",
    "expected_revenue_control = torch.sum(probabilities_control * bundle_prices.expand_as(probabilities_control), dim=0).sum()\n",
    "\n",
    "print(f\"Expected Revenue (Control Group): ${expected_revenue_control.item():.2f}\")\n",
    "\n",
    "# Calculate expected revenue for treated group\n",
    "utilities_treated = model(X_user_test, X_product, price, all_product_treated, bundle_choices)\n",
    "probabilities_treated = F.softmax(utilities_treated, dim=1)\n",
    "expected_revenue_treated = torch.sum(probabilities_treated * bundle_prices.expand_as(probabilities_treated), dim=0).sum()\n",
    "\n",
    "print(f\"Expected Revenue (Treated Group): ${expected_revenue_treated.item():.2f}\")\n",
    "\n",
    "linear = (expected_revenue_treated-expected_revenue_control).cpu().detach().numpy()\n",
    "linear = linear*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "P7Z_BF2C1kdj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7Z_BF2C1kdj",
    "outputId": "6998bb85-85b3-4cf8-da34-4ecded076114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Difference (Treated - Control) by Linear MNL: $-113.28\n",
      "Absolute Percentage Estimation Error of Linear MNL:  -99.26%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Revenue Difference (Treated - Control) by Linear MNL: ${linear:.2f}\")\n",
    "print(f\"Absolute Percentage Estimation Error of Linear MNL:  {100*np.abs(linear-revenue_difference)/revenue_difference:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd460",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# use PDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dada1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2899bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_bundle(user_features, product_features, prices):\n",
    "    # distinct device reference\n",
    "    device = product_features.device\n",
    "    \n",
    "    num_products = product_features.shape[0]\n",
    "    num_bundles = 2 ** num_products\n",
    "    bundle_choices = torch.tensor(\n",
    "        [[int(bit) for bit in np.binary_repr(i, width=num_products)] for i in range(num_bundles)],\n",
    "        dtype=torch.bool,\n",
    "        device=device \n",
    "    )\n",
    "    \n",
    "    # Calculate bundle prices\n",
    "    bundle_prices = torch.tensor([prices[bundle_mask].sum() for bundle_mask in bundle_choices], device=device)\n",
    "\n",
    "    # Initialize lists\n",
    "    all_x_included_products = []\n",
    "    all_x_other_products = []\n",
    "    all_bundle_prices = []\n",
    "    \n",
    "    # Iterate through each bundle\n",
    "    for i, bundle_mask in enumerate(bundle_choices):\n",
    "        # Get included product indices\n",
    "        included_indices = torch.where(bundle_mask)[0]\n",
    "        excluded_indices = torch.where(~bundle_mask)[0]\n",
    "\n",
    "        if included_indices.nelement() > 0:\n",
    "            included_products = product_features[included_indices].reshape(-1)\n",
    "        else:\n",
    "            included_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Features of excluded products\n",
    "        if excluded_indices.nelement() > 0:\n",
    "            other_products = product_features[excluded_indices].reshape(-1)\n",
    "        else:\n",
    "            other_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Price of the current bundle\n",
    "        current_bundle_price = bundle_prices[i]\n",
    "\n",
    "        # Append to lists\n",
    "        all_x_included_products.append(included_products)\n",
    "        all_x_other_products.append(other_products)\n",
    "        all_bundle_prices.append(current_bundle_price)\n",
    "        \n",
    "\n",
    "    max_included_len = max([x.numel() for x in all_x_included_products])\n",
    "    max_other_len = max([x.numel() for x in all_x_other_products])\n",
    "\n",
    "  \n",
    "    all_x_included_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_included_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_included_products\n",
    "    ])\n",
    "\n",
    "    all_x_other_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_other_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_other_products\n",
    "    ])\n",
    "    \n",
    "    all_bundle_prices = torch.stack(all_bundle_prices)\n",
    "\n",
    "    return user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "134acfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)\n",
    "X_user_train1 = X_user_train1.to(device)\n",
    "#for complementarity model\n",
    "prepared_data = prepare_data_bundle(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices= prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39864359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDLModel(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(PDLModel, self).__init__()\n",
    "        # Combined feature dimension includes product features, price, and user features, as well as other products' features and prices\n",
    "        total_feature_dim = user_feature_dim +product_feature_dim*(NUM_Product)+2  # +1 for price\n",
    "\n",
    "        # Single neural network to process the combined features\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1) \n",
    "        )\n",
    "            # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_user, x_product, x_other_products,prices):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        # Process other products' features\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                        x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        aggregated_other_features.unsqueeze(0).expand(N, -1, -1),\n",
    "                                        prices.view(1, -1, 1).expand(N, -1, -1)),\n",
    "                                        dim=2)\n",
    "   \n",
    "\n",
    "        # Compute utility for each combined feature set\n",
    "        utilities = self.network(combined_features).squeeze(-1)\n",
    "\n",
    "\n",
    "        return utilities\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a160035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdlmodel = PDLModel(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1ceb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.7710773944854736, Validation Loss: 2.762986421585083\n",
      "Epoch 2, Training Loss: 2.763418436050415, Validation Loss: 2.7556018829345703\n",
      "Epoch 3, Training Loss: 2.7562291622161865, Validation Loss: 2.747253656387329\n",
      "Epoch 4, Training Loss: 2.7483325004577637, Validation Loss: 2.7366998195648193\n",
      "Epoch 5, Training Loss: 2.7381398677825928, Validation Loss: 2.7237296104431152\n",
      "Epoch 6, Training Loss: 2.7255008220672607, Validation Loss: 2.709301471710205\n",
      "Epoch 7, Training Loss: 2.711395502090454, Validation Loss: 2.6926827430725098\n",
      "Epoch 8, Training Loss: 2.695099353790283, Validation Loss: 2.6736888885498047\n",
      "Epoch 9, Training Loss: 2.6766371726989746, Validation Loss: 2.6526615619659424\n",
      "Epoch 10, Training Loss: 2.656370162963867, Validation Loss: 2.629607915878296\n",
      "Epoch 11, Training Loss: 2.634136915206909, Validation Loss: 2.6068639755249023\n",
      "Epoch 12, Training Loss: 2.6125316619873047, Validation Loss: 2.586130142211914\n",
      "Epoch 13, Training Loss: 2.5927205085754395, Validation Loss: 2.563209056854248\n",
      "Epoch 14, Training Loss: 2.570773124694824, Validation Loss: 2.537980079650879\n",
      "Epoch 15, Training Loss: 2.546335220336914, Validation Loss: 2.5099644660949707\n",
      "Epoch 16, Training Loss: 2.5191962718963623, Validation Loss: 2.4785923957824707\n",
      "Epoch 17, Training Loss: 2.4889066219329834, Validation Loss: 2.443446636199951\n",
      "Epoch 18, Training Loss: 2.4549560546875, Validation Loss: 2.4042468070983887\n",
      "Epoch 19, Training Loss: 2.4170665740966797, Validation Loss: 2.360914945602417\n",
      "Epoch 20, Training Loss: 2.3753528594970703, Validation Loss: 2.3141822814941406\n",
      "Epoch 21, Training Loss: 2.33042049407959, Validation Loss: 2.2671375274658203\n",
      "Epoch 22, Training Loss: 2.2849957942962646, Validation Loss: 2.2198164463043213\n",
      "Epoch 23, Training Loss: 2.2395925521850586, Validation Loss: 2.172945499420166\n",
      "Epoch 24, Training Loss: 2.19466495513916, Validation Loss: 2.127527952194214\n",
      "Epoch 25, Training Loss: 2.1514134407043457, Validation Loss: 2.08459734916687\n",
      "Epoch 26, Training Loss: 2.1109137535095215, Validation Loss: 2.045809030532837\n",
      "Epoch 27, Training Loss: 2.074805498123169, Validation Loss: 2.0130598545074463\n",
      "Epoch 28, Training Loss: 2.044891595840454, Validation Loss: 1.9877188205718994\n",
      "Epoch 29, Training Loss: 2.022432804107666, Validation Loss: 1.970025658607483\n",
      "Epoch 30, Training Loss: 2.007403612136841, Validation Loss: 1.9585973024368286\n",
      "Epoch 31, Training Loss: 1.9982569217681885, Validation Loss: 1.9508726596832275\n",
      "Epoch 32, Training Loss: 1.9922893047332764, Validation Loss: 1.9433470964431763\n",
      "Epoch 33, Training Loss: 1.985763430595398, Validation Loss: 1.9323550462722778\n",
      "Epoch 34, Training Loss: 1.9748945236206055, Validation Loss: 1.9158697128295898\n",
      "Epoch 35, Training Loss: 1.9572097063064575, Validation Loss: 1.8952608108520508\n",
      "Epoch 36, Training Loss: 1.933889389038086, Validation Loss: 1.8728902339935303\n",
      "Epoch 37, Training Loss: 1.9079800844192505, Validation Loss: 1.8514314889907837\n",
      "Epoch 38, Training Loss: 1.8825820684432983, Validation Loss: 1.8337537050247192\n",
      "Epoch 39, Training Loss: 1.8606232404708862, Validation Loss: 1.8219046592712402\n",
      "Epoch 40, Training Loss: 1.8445712327957153, Validation Loss: 1.816245675086975\n",
      "Epoch 41, Training Loss: 1.8348212242126465, Validation Loss: 1.812552809715271\n",
      "Epoch 42, Training Loss: 1.8284587860107422, Validation Loss: 1.8103764057159424\n",
      "Epoch 43, Training Loss: 1.8232556581497192, Validation Loss: 1.810266375541687\n",
      "Epoch 44, Training Loss: 1.8201212882995605, Validation Loss: 1.8107589483261108\n",
      "Epoch 45, Training Loss: 1.8181540966033936, Validation Loss: 1.8103362321853638\n",
      "Epoch 46, Training Loss: 1.8155312538146973, Validation Loss: 1.8077278137207031\n",
      "Epoch 47, Training Loss: 1.811258316040039, Validation Loss: 1.803477168083191\n",
      "Epoch 48, Training Loss: 1.8056933879852295, Validation Loss: 1.7987395524978638\n",
      "Epoch 49, Training Loss: 1.7998054027557373, Validation Loss: 1.7941714525222778\n",
      "Epoch 50, Training Loss: 1.7942856550216675, Validation Loss: 1.789826512336731\n",
      "Epoch 51, Training Loss: 1.7892394065856934, Validation Loss: 1.7857667207717896\n",
      "Epoch 52, Training Loss: 1.784641981124878, Validation Loss: 1.782315969467163\n",
      "Epoch 53, Training Loss: 1.7806974649429321, Validation Loss: 1.779583215713501\n",
      "Epoch 54, Training Loss: 1.7775598764419556, Validation Loss: 1.776934266090393\n",
      "Epoch 55, Training Loss: 1.7746769189834595, Validation Loss: 1.7733712196350098\n",
      "Epoch 56, Training Loss: 1.7710134983062744, Validation Loss: 1.76837956905365\n",
      "Epoch 57, Training Loss: 1.7661396265029907, Validation Loss: 1.7626492977142334\n",
      "Epoch 58, Training Loss: 1.760534405708313, Validation Loss: 1.757142424583435\n",
      "Epoch 59, Training Loss: 1.7550441026687622, Validation Loss: 1.752294659614563\n",
      "Epoch 60, Training Loss: 1.7499079704284668, Validation Loss: 1.7481439113616943\n",
      "Epoch 61, Training Loss: 1.745423674583435, Validation Loss: 1.7447952032089233\n",
      "Epoch 62, Training Loss: 1.7414270639419556, Validation Loss: 1.7420576810836792\n",
      "Epoch 63, Training Loss: 1.7378934621810913, Validation Loss: 1.7396483421325684\n",
      "Epoch 64, Training Loss: 1.7347556352615356, Validation Loss: 1.736652135848999\n",
      "Epoch 65, Training Loss: 1.731203556060791, Validation Loss: 1.732590913772583\n",
      "Epoch 66, Training Loss: 1.726797342300415, Validation Loss: 1.7277369499206543\n",
      "Epoch 67, Training Loss: 1.7217096090316772, Validation Loss: 1.7226358652114868\n",
      "Epoch 68, Training Loss: 1.716483473777771, Validation Loss: 1.7176729440689087\n",
      "Epoch 69, Training Loss: 1.7115509510040283, Validation Loss: 1.7129548788070679\n",
      "Epoch 70, Training Loss: 1.7068278789520264, Validation Loss: 1.7082411050796509\n",
      "Epoch 71, Training Loss: 1.7020573616027832, Validation Loss: 1.7038321495056152\n",
      "Epoch 72, Training Loss: 1.6973878145217896, Validation Loss: 1.6995999813079834\n",
      "Epoch 73, Training Loss: 1.6927205324172974, Validation Loss: 1.6952203512191772\n",
      "Epoch 74, Training Loss: 1.6875804662704468, Validation Loss: 1.6906921863555908\n",
      "Epoch 75, Training Loss: 1.6820223331451416, Validation Loss: 1.6864902973175049\n",
      "Epoch 76, Training Loss: 1.6765081882476807, Validation Loss: 1.68254554271698\n",
      "Epoch 77, Training Loss: 1.6711567640304565, Validation Loss: 1.6784862279891968\n",
      "Epoch 78, Training Loss: 1.6656997203826904, Validation Loss: 1.6745400428771973\n",
      "Epoch 79, Training Loss: 1.6604818105697632, Validation Loss: 1.6707980632781982\n",
      "Epoch 80, Training Loss: 1.6554678678512573, Validation Loss: 1.6670072078704834\n",
      "Epoch 81, Training Loss: 1.6503576040267944, Validation Loss: 1.6630096435546875\n",
      "Epoch 82, Training Loss: 1.6451150178909302, Validation Loss: 1.6588671207427979\n",
      "Epoch 83, Training Loss: 1.6397701501846313, Validation Loss: 1.6548155546188354\n",
      "Epoch 84, Training Loss: 1.634590744972229, Validation Loss: 1.650986909866333\n",
      "Epoch 85, Training Loss: 1.629648208618164, Validation Loss: 1.6475318670272827\n",
      "Epoch 86, Training Loss: 1.6249613761901855, Validation Loss: 1.6442993879318237\n",
      "Epoch 87, Training Loss: 1.6204288005828857, Validation Loss: 1.641367793083191\n",
      "Epoch 88, Training Loss: 1.616019606590271, Validation Loss: 1.6386781930923462\n",
      "Epoch 89, Training Loss: 1.6115458011627197, Validation Loss: 1.6363199949264526\n",
      "Epoch 90, Training Loss: 1.607078194618225, Validation Loss: 1.634117841720581\n",
      "Epoch 91, Training Loss: 1.6027151346206665, Validation Loss: 1.6320483684539795\n",
      "Epoch 92, Training Loss: 1.5984318256378174, Validation Loss: 1.629737138748169\n",
      "Epoch 93, Training Loss: 1.594193935394287, Validation Loss: 1.627445936203003\n",
      "Epoch 94, Training Loss: 1.5900720357894897, Validation Loss: 1.625477910041809\n",
      "Epoch 95, Training Loss: 1.586160659790039, Validation Loss: 1.6233766078948975\n",
      "Epoch 96, Training Loss: 1.5822277069091797, Validation Loss: 1.620259404182434\n",
      "Epoch 97, Training Loss: 1.5784322023391724, Validation Loss: 1.6169917583465576\n",
      "Epoch 98, Training Loss: 1.5749973058700562, Validation Loss: 1.6140120029449463\n",
      "Epoch 99, Training Loss: 1.571449875831604, Validation Loss: 1.6110255718231201\n",
      "Epoch 100, Training Loss: 1.567962408065796, Validation Loss: 1.6081064939498901\n",
      "Epoch 101, Training Loss: 1.564585566520691, Validation Loss: 1.6053228378295898\n",
      "Epoch 102, Training Loss: 1.5612926483154297, Validation Loss: 1.6027361154556274\n",
      "Epoch 103, Training Loss: 1.5580397844314575, Validation Loss: 1.6001849174499512\n",
      "Epoch 104, Training Loss: 1.5548104047775269, Validation Loss: 1.5977604389190674\n",
      "Epoch 105, Training Loss: 1.5516597032546997, Validation Loss: 1.59563148021698\n",
      "Epoch 106, Training Loss: 1.5486935377120972, Validation Loss: 1.5937585830688477\n",
      "Epoch 107, Training Loss: 1.5459264516830444, Validation Loss: 1.5920895338058472\n",
      "Epoch 108, Training Loss: 1.5433140993118286, Validation Loss: 1.5902760028839111\n",
      "Epoch 109, Training Loss: 1.5407793521881104, Validation Loss: 1.588521957397461\n",
      "Epoch 110, Training Loss: 1.5384474992752075, Validation Loss: 1.5868200063705444\n",
      "Epoch 111, Training Loss: 1.5364069938659668, Validation Loss: 1.5851553678512573\n",
      "Epoch 112, Training Loss: 1.534468173980713, Validation Loss: 1.5832563638687134\n",
      "Epoch 113, Training Loss: 1.5325301885604858, Validation Loss: 1.581830382347107\n",
      "Epoch 114, Training Loss: 1.5307261943817139, Validation Loss: 1.5811692476272583\n",
      "Epoch 115, Training Loss: 1.5291317701339722, Validation Loss: 1.5807369947433472\n",
      "Epoch 116, Training Loss: 1.5275161266326904, Validation Loss: 1.5804942846298218\n",
      "Epoch 117, Training Loss: 1.52590811252594, Validation Loss: 1.5803972482681274\n",
      "Epoch 118, Training Loss: 1.5245232582092285, Validation Loss: 1.5801368951797485\n",
      "Epoch 119, Training Loss: 1.5232352018356323, Validation Loss: 1.579569697380066\n",
      "Epoch 120, Training Loss: 1.5219125747680664, Validation Loss: 1.5789332389831543\n",
      "Epoch 121, Training Loss: 1.5206393003463745, Validation Loss: 1.5781745910644531\n",
      "Epoch 122, Training Loss: 1.5194660425186157, Validation Loss: 1.5770204067230225\n",
      "Epoch 123, Training Loss: 1.518246054649353, Validation Loss: 1.5758030414581299\n",
      "Epoch 124, Training Loss: 1.5171048641204834, Validation Loss: 1.574643611907959\n",
      "Epoch 125, Training Loss: 1.5160021781921387, Validation Loss: 1.5734249353408813\n",
      "Epoch 126, Training Loss: 1.5148224830627441, Validation Loss: 1.5720313787460327\n",
      "Epoch 127, Training Loss: 1.513645052909851, Validation Loss: 1.5706919431686401\n",
      "Epoch 128, Training Loss: 1.5125969648361206, Validation Loss: 1.5694262981414795\n",
      "Epoch 129, Training Loss: 1.5115712881088257, Validation Loss: 1.5681755542755127\n",
      "Epoch 130, Training Loss: 1.5105329751968384, Validation Loss: 1.5668208599090576\n",
      "Epoch 131, Training Loss: 1.5095317363739014, Validation Loss: 1.5652735233306885\n",
      "Epoch 132, Training Loss: 1.5084952116012573, Validation Loss: 1.5637205839157104\n",
      "Epoch 133, Training Loss: 1.5074429512023926, Validation Loss: 1.5623588562011719\n",
      "Epoch 134, Training Loss: 1.5064672231674194, Validation Loss: 1.561034083366394\n",
      "Epoch 135, Training Loss: 1.5054728984832764, Validation Loss: 1.5596436262130737\n",
      "Epoch 136, Training Loss: 1.5044833421707153, Validation Loss: 1.5582356452941895\n",
      "Epoch 137, Training Loss: 1.5035314559936523, Validation Loss: 1.5569325685501099\n",
      "Epoch 138, Training Loss: 1.5025683641433716, Validation Loss: 1.5559210777282715\n",
      "Epoch 139, Training Loss: 1.501637578010559, Validation Loss: 1.5550410747528076\n",
      "Epoch 140, Training Loss: 1.5007355213165283, Validation Loss: 1.5538702011108398\n",
      "Epoch 141, Training Loss: 1.4998140335083008, Validation Loss: 1.5529166460037231\n",
      "Epoch 142, Training Loss: 1.4989310503005981, Validation Loss: 1.5520868301391602\n",
      "Epoch 143, Training Loss: 1.498052716255188, Validation Loss: 1.5512166023254395\n",
      "Epoch 144, Training Loss: 1.4971824884414673, Validation Loss: 1.550241231918335\n",
      "Epoch 145, Training Loss: 1.4963325262069702, Validation Loss: 1.5495779514312744\n",
      "Epoch 146, Training Loss: 1.4954878091812134, Validation Loss: 1.5491437911987305\n",
      "Epoch 147, Training Loss: 1.49464750289917, Validation Loss: 1.548592209815979\n",
      "Epoch 148, Training Loss: 1.4938677549362183, Validation Loss: 1.547727346420288\n",
      "Epoch 149, Training Loss: 1.4930700063705444, Validation Loss: 1.5470913648605347\n",
      "Epoch 150, Training Loss: 1.4923394918441772, Validation Loss: 1.5466033220291138\n",
      "Epoch 151, Training Loss: 1.4915924072265625, Validation Loss: 1.5460423231124878\n",
      "Epoch 152, Training Loss: 1.4908570051193237, Validation Loss: 1.5454777479171753\n",
      "Epoch 153, Training Loss: 1.4901319742202759, Validation Loss: 1.544915795326233\n",
      "Epoch 154, Training Loss: 1.489410161972046, Validation Loss: 1.5443216562271118\n",
      "Epoch 155, Training Loss: 1.4886833429336548, Validation Loss: 1.5435552597045898\n",
      "Epoch 156, Training Loss: 1.4879450798034668, Validation Loss: 1.542689323425293\n",
      "Epoch 157, Training Loss: 1.487202763557434, Validation Loss: 1.541879415512085\n",
      "Epoch 158, Training Loss: 1.4864933490753174, Validation Loss: 1.5412124395370483\n",
      "Epoch 159, Training Loss: 1.4857847690582275, Validation Loss: 1.5405203104019165\n",
      "Epoch 160, Training Loss: 1.4850835800170898, Validation Loss: 1.539806604385376\n",
      "Epoch 161, Training Loss: 1.484427571296692, Validation Loss: 1.5390499830245972\n",
      "Epoch 162, Training Loss: 1.4837568998336792, Validation Loss: 1.5382572412490845\n",
      "Epoch 163, Training Loss: 1.4831103086471558, Validation Loss: 1.5374035835266113\n",
      "Epoch 164, Training Loss: 1.4824904203414917, Validation Loss: 1.5365768671035767\n",
      "Epoch 165, Training Loss: 1.4818599224090576, Validation Loss: 1.5358363389968872\n",
      "Epoch 166, Training Loss: 1.481205940246582, Validation Loss: 1.5353118181228638\n",
      "Epoch 167, Training Loss: 1.4805861711502075, Validation Loss: 1.5348987579345703\n",
      "Epoch 168, Training Loss: 1.4799468517303467, Validation Loss: 1.5345228910446167\n",
      "Epoch 169, Training Loss: 1.4793192148208618, Validation Loss: 1.5341472625732422\n",
      "Epoch 170, Training Loss: 1.478686809539795, Validation Loss: 1.5337388515472412\n",
      "Epoch 171, Training Loss: 1.4780991077423096, Validation Loss: 1.5331885814666748\n",
      "Epoch 172, Training Loss: 1.4775316715240479, Validation Loss: 1.5327610969543457\n",
      "Epoch 173, Training Loss: 1.4769279956817627, Validation Loss: 1.531963586807251\n",
      "Epoch 174, Training Loss: 1.4762705564498901, Validation Loss: 1.5312076807022095\n",
      "Epoch 175, Training Loss: 1.4756652116775513, Validation Loss: 1.530739665031433\n",
      "Epoch 176, Training Loss: 1.4750957489013672, Validation Loss: 1.5299975872039795\n",
      "Epoch 177, Training Loss: 1.4745293855667114, Validation Loss: 1.5295050144195557\n",
      "Epoch 178, Training Loss: 1.4739453792572021, Validation Loss: 1.528395175933838\n",
      "Epoch 179, Training Loss: 1.4733669757843018, Validation Loss: 1.5279439687728882\n",
      "Epoch 180, Training Loss: 1.472760558128357, Validation Loss: 1.5269761085510254\n",
      "Epoch 181, Training Loss: 1.472184419631958, Validation Loss: 1.526406168937683\n",
      "Epoch 182, Training Loss: 1.4716339111328125, Validation Loss: 1.5252591371536255\n",
      "Epoch 183, Training Loss: 1.4710782766342163, Validation Loss: 1.524819016456604\n",
      "Epoch 184, Training Loss: 1.470521330833435, Validation Loss: 1.5239450931549072\n",
      "Epoch 185, Training Loss: 1.469970464706421, Validation Loss: 1.5234465599060059\n",
      "Epoch 186, Training Loss: 1.469408631324768, Validation Loss: 1.5228283405303955\n",
      "Epoch 187, Training Loss: 1.4688607454299927, Validation Loss: 1.52214777469635\n",
      "Epoch 188, Training Loss: 1.468345046043396, Validation Loss: 1.5216140747070312\n",
      "Epoch 189, Training Loss: 1.4678272008895874, Validation Loss: 1.5205403566360474\n",
      "Epoch 190, Training Loss: 1.4674102067947388, Validation Loss: 1.520463228225708\n",
      "Epoch 191, Training Loss: 1.4670261144638062, Validation Loss: 1.5189921855926514\n",
      "Epoch 192, Training Loss: 1.4665071964263916, Validation Loss: 1.5186316967010498\n",
      "Epoch 193, Training Loss: 1.4657608270645142, Validation Loss: 1.517339825630188\n",
      "Epoch 194, Training Loss: 1.4650644063949585, Validation Loss: 1.5167664289474487\n",
      "Epoch 195, Training Loss: 1.464445948600769, Validation Loss: 1.5160077810287476\n",
      "Epoch 196, Training Loss: 1.4638655185699463, Validation Loss: 1.5153931379318237\n",
      "Epoch 197, Training Loss: 1.4633278846740723, Validation Loss: 1.5151915550231934\n",
      "Epoch 198, Training Loss: 1.462823748588562, Validation Loss: 1.5142449140548706\n",
      "Epoch 199, Training Loss: 1.4622972011566162, Validation Loss: 1.5143541097640991\n",
      "Epoch 200, Training Loss: 1.461849570274353, Validation Loss: 1.5128895044326782\n",
      "Epoch 201, Training Loss: 1.461219072341919, Validation Loss: 1.5128117799758911\n",
      "Epoch 202, Training Loss: 1.4606411457061768, Validation Loss: 1.511763572692871\n",
      "Epoch 203, Training Loss: 1.4600106477737427, Validation Loss: 1.5111418962478638\n",
      "Epoch 204, Training Loss: 1.4595158100128174, Validation Loss: 1.5111570358276367\n",
      "Epoch 205, Training Loss: 1.4590564966201782, Validation Loss: 1.5104318857192993\n",
      "Epoch 206, Training Loss: 1.4585745334625244, Validation Loss: 1.5109390020370483\n",
      "Epoch 207, Training Loss: 1.458145022392273, Validation Loss: 1.5102717876434326\n",
      "Epoch 208, Training Loss: 1.457751989364624, Validation Loss: 1.5112261772155762\n",
      "Epoch 209, Training Loss: 1.4575196504592896, Validation Loss: 1.5095314979553223\n",
      "Epoch 210, Training Loss: 1.4568880796432495, Validation Loss: 1.50973379611969\n",
      "Epoch 211, Training Loss: 1.4561388492584229, Validation Loss: 1.5077893733978271\n",
      "Epoch 212, Training Loss: 1.4551846981048584, Validation Loss: 1.5072120428085327\n",
      "Epoch 213, Training Loss: 1.4544223546981812, Validation Loss: 1.506700873374939\n",
      "Epoch 214, Training Loss: 1.4538630247116089, Validation Loss: 1.505693793296814\n",
      "Epoch 215, Training Loss: 1.453546166419983, Validation Loss: 1.5063227415084839\n",
      "Epoch 216, Training Loss: 1.4531056880950928, Validation Loss: 1.5044105052947998\n",
      "Epoch 217, Training Loss: 1.4520255327224731, Validation Loss: 1.503846526145935\n",
      "Epoch 218, Training Loss: 1.450982689857483, Validation Loss: 1.502859354019165\n",
      "Epoch 219, Training Loss: 1.4501396417617798, Validation Loss: 1.5016200542449951\n",
      "Epoch 220, Training Loss: 1.4493815898895264, Validation Loss: 1.5013889074325562\n",
      "Epoch 221, Training Loss: 1.448570728302002, Validation Loss: 1.4993935823440552\n",
      "Epoch 222, Training Loss: 1.4475560188293457, Validation Loss: 1.4987510442733765\n",
      "Epoch 223, Training Loss: 1.4464519023895264, Validation Loss: 1.4972524642944336\n",
      "Epoch 224, Training Loss: 1.4454034566879272, Validation Loss: 1.4961143732070923\n",
      "Epoch 225, Training Loss: 1.4443790912628174, Validation Loss: 1.494981288909912\n",
      "Epoch 226, Training Loss: 1.4433608055114746, Validation Loss: 1.4938533306121826\n",
      "Epoch 227, Training Loss: 1.4423454999923706, Validation Loss: 1.4928820133209229\n",
      "Epoch 228, Training Loss: 1.441372036933899, Validation Loss: 1.4918500185012817\n",
      "Epoch 229, Training Loss: 1.4404090642929077, Validation Loss: 1.4907692670822144\n",
      "Epoch 230, Training Loss: 1.439450979232788, Validation Loss: 1.4893018007278442\n",
      "Epoch 231, Training Loss: 1.4385417699813843, Validation Loss: 1.4885554313659668\n",
      "Epoch 232, Training Loss: 1.4376375675201416, Validation Loss: 1.4865866899490356\n",
      "Epoch 233, Training Loss: 1.436773657798767, Validation Loss: 1.4863860607147217\n",
      "Epoch 234, Training Loss: 1.4360603094100952, Validation Loss: 1.4838435649871826\n",
      "Epoch 235, Training Loss: 1.4350335597991943, Validation Loss: 1.4831701517105103\n",
      "Epoch 236, Training Loss: 1.4340236186981201, Validation Loss: 1.4817390441894531\n",
      "Epoch 237, Training Loss: 1.4331625699996948, Validation Loss: 1.4805874824523926\n",
      "Epoch 238, Training Loss: 1.4324817657470703, Validation Loss: 1.4803738594055176\n",
      "Epoch 239, Training Loss: 1.4318397045135498, Validation Loss: 1.4790364503860474\n",
      "Epoch 240, Training Loss: 1.4310551881790161, Validation Loss: 1.4782769680023193\n",
      "Epoch 241, Training Loss: 1.4303032159805298, Validation Loss: 1.4772002696990967\n",
      "Epoch 242, Training Loss: 1.4296011924743652, Validation Loss: 1.4760420322418213\n",
      "Epoch 243, Training Loss: 1.4290037155151367, Validation Loss: 1.4755191802978516\n",
      "Epoch 244, Training Loss: 1.4285387992858887, Validation Loss: 1.4750099182128906\n",
      "Epoch 245, Training Loss: 1.4281545877456665, Validation Loss: 1.4749561548233032\n",
      "Epoch 246, Training Loss: 1.4276710748672485, Validation Loss: 1.474355697631836\n",
      "Epoch 247, Training Loss: 1.4271633625030518, Validation Loss: 1.4743155241012573\n",
      "Epoch 248, Training Loss: 1.4264930486679077, Validation Loss: 1.4742958545684814\n",
      "Epoch 249, Training Loss: 1.426066517829895, Validation Loss: 1.4738202095031738\n",
      "Epoch 250, Training Loss: 1.4258801937103271, Validation Loss: 1.473964810371399\n",
      "Epoch 251, Training Loss: 1.4258134365081787, Validation Loss: 1.4728621244430542\n",
      "Epoch 252, Training Loss: 1.425703763961792, Validation Loss: 1.4731864929199219\n",
      "Epoch 253, Training Loss: 1.4251779317855835, Validation Loss: 1.4721499681472778\n",
      "Epoch 254, Training Loss: 1.4247111082077026, Validation Loss: 1.471823811531067\n",
      "Epoch 255, Training Loss: 1.4244054555892944, Validation Loss: 1.4718494415283203\n",
      "Epoch 256, Training Loss: 1.42436945438385, Validation Loss: 1.4708812236785889\n",
      "Epoch 257, Training Loss: 1.4242573976516724, Validation Loss: 1.4713375568389893\n",
      "Epoch 258, Training Loss: 1.4239393472671509, Validation Loss: 1.4704362154006958\n",
      "Epoch 259, Training Loss: 1.4236037731170654, Validation Loss: 1.470238208770752\n",
      "Epoch 260, Training Loss: 1.4234929084777832, Validation Loss: 1.4702751636505127\n",
      "Epoch 261, Training Loss: 1.4233485460281372, Validation Loss: 1.469123125076294\n",
      "Epoch 262, Training Loss: 1.4231414794921875, Validation Loss: 1.469330072402954\n",
      "Epoch 263, Training Loss: 1.4229423999786377, Validation Loss: 1.4696083068847656\n",
      "Epoch 264, Training Loss: 1.4228073358535767, Validation Loss: 1.4690693616867065\n",
      "Epoch 265, Training Loss: 1.4227098226547241, Validation Loss: 1.470407485961914\n",
      "Epoch 266, Training Loss: 1.4225537776947021, Validation Loss: 1.469346523284912\n",
      "Epoch 267, Training Loss: 1.4222757816314697, Validation Loss: 1.4691411256790161\n",
      "Epoch 268, Training Loss: 1.4221222400665283, Validation Loss: 1.469598412513733\n",
      "Epoch 269, Training Loss: 1.4220855236053467, Validation Loss: 1.4685540199279785\n",
      "Epoch 270, Training Loss: 1.4218982458114624, Validation Loss: 1.4691985845565796\n",
      "Epoch 271, Training Loss: 1.4217184782028198, Validation Loss: 1.4689610004425049\n",
      "Epoch 272, Training Loss: 1.4215933084487915, Validation Loss: 1.4682033061981201\n",
      "Epoch 273, Training Loss: 1.4215449094772339, Validation Loss: 1.4691792726516724\n",
      "Epoch 274, Training Loss: 1.4214948415756226, Validation Loss: 1.4682543277740479\n",
      "Epoch 275, Training Loss: 1.4213290214538574, Validation Loss: 1.468699336051941\n",
      "Epoch 276, Training Loss: 1.4212170839309692, Validation Loss: 1.469490885734558\n",
      "Epoch 277, Training Loss: 1.4211682081222534, Validation Loss: 1.4683146476745605\n",
      "Epoch 278, Training Loss: 1.421059012413025, Validation Loss: 1.4691450595855713\n",
      "Epoch 279, Training Loss: 1.4209729433059692, Validation Loss: 1.469016194343567\n",
      "Epoch 280, Training Loss: 1.4209060668945312, Validation Loss: 1.4688664674758911\n",
      "Epoch 281, Training Loss: 1.4208173751831055, Validation Loss: 1.4699032306671143\n",
      "Epoch 282, Training Loss: 1.4207147359848022, Validation Loss: 1.4692089557647705\n",
      "Epoch 283, Training Loss: 1.420559048652649, Validation Loss: 1.4693001508712769\n",
      "Epoch 284, Training Loss: 1.4204515218734741, Validation Loss: 1.4694526195526123\n",
      "Epoch 285, Training Loss: 1.4203789234161377, Validation Loss: 1.4689229726791382\n",
      "Epoch 286, Training Loss: 1.420337200164795, Validation Loss: 1.4701471328735352\n",
      "Epoch 287, Training Loss: 1.4203131198883057, Validation Loss: 1.469290852546692\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(pdlmodel.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pdlmodel.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    outputs = pdlmodel(user_features, all_x_included_products, all_x_other_products,all_bundle_prices)\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    pdlmodel.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = pdlmodel(X_user_val,  all_x_included_products, all_x_other_products ,all_bundle_prices)\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b5374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for complementarity model\n",
    "def calculate_expected_revenue(model, user_features, all_x_included_products, bundle_prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Calculate utilities for all bundles\n",
    "        utilities = model(user_features, all_x_included_products, all_x_other_products,bundle_prices)\n",
    "        probabilities = F.softmax(utilities, dim=1)\n",
    "\n",
    "        # Calculate total expected revenue\n",
    "        total_expected_revenue = (probabilities * bundle_prices.unsqueeze(0)).sum()\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c915a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $10209.66\n",
      "Expected Revenue all treated: $2104.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8105.18017578125"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data_bundle(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products,all_bundle_prices = control_prepared_data\n",
    "expected_revenue_all_control = calculate_expected_revenue(pdlmodel, user_features, all_x_included_products, all_bundle_prices, )\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data_bundle(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(pdlmodel, user_features, all_x_included_products, all_bundle_prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")\n",
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17cb1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdl = (expected_revenue_all_treated-expected_revenue_all_control)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a4df632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of PDL:  -6.20%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of PDL:  {100*np.abs(pdl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c900cf",
   "metadata": {
    "id": "63c900cf"
   },
   "source": [
    "# use dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d442cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_user_train, X_product, price = X_user_train.to(device), X_product.to(device), price.to(device)\n",
    "if isinstance(user_randomization, np.ndarray):\n",
    "    user_randomization = torch.from_numpy(user_randomization).to(X_user_train.device).bool()\n",
    "if isinstance(prod_randomization, np.ndarray):\n",
    "    prod_randomization = torch.from_numpy(prod_randomization).to(X_user_train.device).bool()\n",
    "\n",
    "decision_train = decision_train.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa2df020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityEstimator(nn.Module):\n",
    "    def __init__(self, user_feature_dim, product_feature_dim):\n",
    "        super(UtilityEstimator, self).__init__()\n",
    "        \n",
    "        # Layers to process other products' features (z-j)\n",
    "        self.other_product_features_layers = nn.Sequential(\n",
    "            nn.Linear(product_feature_dim*(NUM_Product), NUM_Product),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_Product, 1)\n",
    "        )\n",
    "\n",
    "        self.theta0 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim +product_feature_dim*(NUM_Product)+1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "       \n",
    "        self.theta1 = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim + product_feature_dim*(NUM_Product)+1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_user, x_product, x_other_products,price):\n",
    "        N = x_user.shape[0]\n",
    "        M = x_product.shape[0]\n",
    "        aggregated_other_features = self.other_product_features_layers(x_other_products)\n",
    "\n",
    "        \n",
    "        combined_features_theta =  torch.cat((x_user.unsqueeze(1).expand(-1, M, -1),\n",
    "                                               x_product.unsqueeze(0).expand(N, -1, -1),\n",
    "                                               aggregated_other_features.unsqueeze(0).expand(N, -1, -1)),\n",
    "                                                 dim=2)\n",
    "        theta0_output = self.theta0(combined_features_theta).squeeze(-1)\n",
    "        theta1_output = self.theta1(combined_features_theta).squeeze(-1)\n",
    "        \n",
    "        price = price.unsqueeze(-1)  \n",
    "        utility = theta0_output + theta1_output * price.squeeze(-1)\n",
    "\n",
    "\n",
    "        \n",
    "        return utility,theta0_output,theta1_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf77eeb6",
   "metadata": {
    "id": "cf77eeb6"
   },
   "outputs": [],
   "source": [
    "dml_model = UtilityEstimator(user_feature_dim=USER_Cont_FEATURES+USER_Dicr_FEATURES,\n",
    "                       product_feature_dim=Product_Cont_FEATURES+Product_Dicr_FEATURES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "QT_wrrh3rIws",
   "metadata": {
    "id": "QT_wrrh3rIws"
   },
   "outputs": [],
   "source": [
    "X_user_train1, X_user_val, decision_train1,decision_val = train_test_split(X_user_train,decision_train,test_size=0.1,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31b522ed-0c36-4199-a309-74f71aece365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_bundle(user_features, product_features, prices):\n",
    "    # distinct device reference\n",
    "    device = product_features.device\n",
    "    \n",
    "    num_products = product_features.shape[0]\n",
    "    num_bundles = 2 ** num_products\n",
    "    bundle_choices = torch.tensor(\n",
    "        [[int(bit) for bit in np.binary_repr(i, width=num_products)] for i in range(num_bundles)],\n",
    "        dtype=torch.bool,\n",
    "        device=device \n",
    "    )\n",
    "    \n",
    "    # Calculate bundle prices\n",
    "    bundle_prices = torch.tensor([prices[bundle_mask].sum() for bundle_mask in bundle_choices], device=device)\n",
    "\n",
    "    # Initialize lists\n",
    "    all_x_included_products = []\n",
    "    all_x_other_products = []\n",
    "    all_bundle_prices = []\n",
    "    \n",
    "    # Iterate through each bundle\n",
    "    for i, bundle_mask in enumerate(bundle_choices):\n",
    "        # Get included product indices\n",
    "        included_indices = torch.where(bundle_mask)[0]\n",
    "        excluded_indices = torch.where(~bundle_mask)[0]\n",
    "\n",
    "        if included_indices.nelement() > 0:\n",
    "            included_products = product_features[included_indices].reshape(-1)\n",
    "        else:\n",
    "            included_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Features of excluded products\n",
    "        if excluded_indices.nelement() > 0:\n",
    "            other_products = product_features[excluded_indices].reshape(-1)\n",
    "        else:\n",
    "            other_products = torch.tensor([], dtype=product_features.dtype, device=device)\n",
    "\n",
    "        # Price of the current bundle\n",
    "        current_bundle_price = bundle_prices[i]\n",
    "\n",
    "        # Append to lists\n",
    "        all_x_included_products.append(included_products)\n",
    "        all_x_other_products.append(other_products)\n",
    "        all_bundle_prices.append(current_bundle_price)\n",
    "        \n",
    "\n",
    "    max_included_len = max([x.numel() for x in all_x_included_products])\n",
    "    max_other_len = max([x.numel() for x in all_x_other_products])\n",
    "\n",
    "  \n",
    "    all_x_included_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_included_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_included_products\n",
    "    ])\n",
    "\n",
    "    all_x_other_products = torch.stack([\n",
    "        torch.cat([\n",
    "            x, \n",
    "            torch.zeros(max_other_len - x.numel(), device=device, dtype=x.dtype)\n",
    "        ]) for x in all_x_other_products\n",
    "    ])\n",
    "    \n",
    "    all_bundle_prices = torch.stack(all_bundle_prices)\n",
    "\n",
    "    return user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0ff32e5-5c64-49ad-bbf0-f6aa2a53d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)\n",
    "X_user_train1 = X_user_train1.to(device)\n",
    "#for complementarity model\n",
    "prepared_data = prepare_data_bundle(X_user_train1, X_product,  price * (1 - (1-discount) * prod_randomization))\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices= prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b386e12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b386e12",
    "outputId": "aab71afc-5217-4c73-eba6-26cb31bed335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.9187653064727783, Validation Loss: 2.8863701820373535\n",
      "Epoch 2, Training Loss: 2.8834474086761475, Validation Loss: 2.852341651916504\n",
      "Epoch 3, Training Loss: 2.8501596450805664, Validation Loss: 2.8190836906433105\n",
      "Epoch 4, Training Loss: 2.817775011062622, Validation Loss: 2.783125638961792\n",
      "Epoch 5, Training Loss: 2.782428026199341, Validation Loss: 2.743191957473755\n",
      "Epoch 6, Training Loss: 2.7428579330444336, Validation Loss: 2.698638439178467\n",
      "Epoch 7, Training Loss: 2.698559522628784, Validation Loss: 2.649301767349243\n",
      "Epoch 8, Training Loss: 2.65019154548645, Validation Loss: 2.595186233520508\n",
      "Epoch 9, Training Loss: 2.597923755645752, Validation Loss: 2.5369913578033447\n",
      "Epoch 10, Training Loss: 2.54122257232666, Validation Loss: 2.4749250411987305\n",
      "Epoch 11, Training Loss: 2.481414794921875, Validation Loss: 2.412259340286255\n",
      "Epoch 12, Training Loss: 2.420891046524048, Validation Loss: 2.351177453994751\n",
      "Epoch 13, Training Loss: 2.3618080615997314, Validation Loss: 2.293789863586426\n",
      "Epoch 14, Training Loss: 2.3057570457458496, Validation Loss: 2.238037109375\n",
      "Epoch 15, Training Loss: 2.251202344894409, Validation Loss: 2.18188738822937\n",
      "Epoch 16, Training Loss: 2.196622848510742, Validation Loss: 2.1277012825012207\n",
      "Epoch 17, Training Loss: 2.1441335678100586, Validation Loss: 2.077867269515991\n",
      "Epoch 18, Training Loss: 2.096325635910034, Validation Loss: 2.0354135036468506\n",
      "Epoch 19, Training Loss: 2.0560665130615234, Validation Loss: 2.000774621963501\n",
      "Epoch 20, Training Loss: 2.0233154296875, Validation Loss: 1.9739199876785278\n",
      "Epoch 21, Training Loss: 1.9982560873031616, Validation Loss: 1.954282522201538\n",
      "Epoch 22, Training Loss: 1.98007071018219, Validation Loss: 1.9403468370437622\n",
      "Epoch 23, Training Loss: 1.9672425985336304, Validation Loss: 1.9298635721206665\n",
      "Epoch 24, Training Loss: 1.9572656154632568, Validation Loss: 1.9197105169296265\n",
      "Epoch 25, Training Loss: 1.9470089673995972, Validation Loss: 1.9076721668243408\n",
      "Epoch 26, Training Loss: 1.9341517686843872, Validation Loss: 1.8927967548370361\n",
      "Epoch 27, Training Loss: 1.9178824424743652, Validation Loss: 1.8756190538406372\n",
      "Epoch 28, Training Loss: 1.8991363048553467, Validation Loss: 1.8577377796173096\n",
      "Epoch 29, Training Loss: 1.8794561624526978, Validation Loss: 1.841632604598999\n",
      "Epoch 30, Training Loss: 1.8606998920440674, Validation Loss: 1.8287087678909302\n",
      "Epoch 31, Training Loss: 1.8448641300201416, Validation Loss: 1.8215628862380981\n",
      "Epoch 32, Training Loss: 1.834296464920044, Validation Loss: 1.8172160387039185\n",
      "Epoch 33, Training Loss: 1.8272624015808105, Validation Loss: 1.8110342025756836\n",
      "Epoch 34, Training Loss: 1.8189769983291626, Validation Loss: 1.802945613861084\n",
      "Epoch 35, Training Loss: 1.8090004920959473, Validation Loss: 1.7957484722137451\n",
      "Epoch 36, Training Loss: 1.7999815940856934, Validation Loss: 1.7900590896606445\n",
      "Epoch 37, Training Loss: 1.7936429977416992, Validation Loss: 1.7860110998153687\n",
      "Epoch 38, Training Loss: 1.78959321975708, Validation Loss: 1.7821723222732544\n",
      "Epoch 39, Training Loss: 1.7855772972106934, Validation Loss: 1.7777585983276367\n",
      "Epoch 40, Training Loss: 1.7805664539337158, Validation Loss: 1.7723206281661987\n",
      "Epoch 41, Training Loss: 1.774251937866211, Validation Loss: 1.7660071849822998\n",
      "Epoch 42, Training Loss: 1.7667038440704346, Validation Loss: 1.7597359418869019\n",
      "Epoch 43, Training Loss: 1.758582353591919, Validation Loss: 1.7531429529190063\n",
      "Epoch 44, Training Loss: 1.750019907951355, Validation Loss: 1.7465242147445679\n",
      "Epoch 45, Training Loss: 1.7417584657669067, Validation Loss: 1.740329384803772\n",
      "Epoch 46, Training Loss: 1.73442542552948, Validation Loss: 1.734870195388794\n",
      "Epoch 47, Training Loss: 1.7279301881790161, Validation Loss: 1.7294583320617676\n",
      "Epoch 48, Training Loss: 1.7217180728912354, Validation Loss: 1.7238733768463135\n",
      "Epoch 49, Training Loss: 1.7154510021209717, Validation Loss: 1.7178720235824585\n",
      "Epoch 50, Training Loss: 1.7090203762054443, Validation Loss: 1.7117153406143188\n",
      "Epoch 51, Training Loss: 1.7026492357254028, Validation Loss: 1.7052370309829712\n",
      "Epoch 52, Training Loss: 1.6958820819854736, Validation Loss: 1.6989129781723022\n",
      "Epoch 53, Training Loss: 1.6893532276153564, Validation Loss: 1.692526936531067\n",
      "Epoch 54, Training Loss: 1.682442545890808, Validation Loss: 1.6867188215255737\n",
      "Epoch 55, Training Loss: 1.6753225326538086, Validation Loss: 1.6821016073226929\n",
      "Epoch 56, Training Loss: 1.6692060232162476, Validation Loss: 1.6773042678833008\n",
      "Epoch 57, Training Loss: 1.6633012294769287, Validation Loss: 1.6723309755325317\n",
      "Epoch 58, Training Loss: 1.6579017639160156, Validation Loss: 1.6675456762313843\n",
      "Epoch 59, Training Loss: 1.6528123617172241, Validation Loss: 1.6616860628128052\n",
      "Epoch 60, Training Loss: 1.6461763381958008, Validation Loss: 1.6563960313796997\n",
      "Epoch 61, Training Loss: 1.6401318311691284, Validation Loss: 1.6512157917022705\n",
      "Epoch 62, Training Loss: 1.6343506574630737, Validation Loss: 1.645986557006836\n",
      "Epoch 63, Training Loss: 1.628405213356018, Validation Loss: 1.6409847736358643\n",
      "Epoch 64, Training Loss: 1.622341513633728, Validation Loss: 1.6361284255981445\n",
      "Epoch 65, Training Loss: 1.6162770986557007, Validation Loss: 1.631654143333435\n",
      "Epoch 66, Training Loss: 1.6103886365890503, Validation Loss: 1.627482295036316\n",
      "Epoch 67, Training Loss: 1.6054247617721558, Validation Loss: 1.6232800483703613\n",
      "Epoch 68, Training Loss: 1.6004828214645386, Validation Loss: 1.619086503982544\n",
      "Epoch 69, Training Loss: 1.5954416990280151, Validation Loss: 1.6147749423980713\n",
      "Epoch 70, Training Loss: 1.590306282043457, Validation Loss: 1.610080599784851\n",
      "Epoch 71, Training Loss: 1.5852956771850586, Validation Loss: 1.6050677299499512\n",
      "Epoch 72, Training Loss: 1.5805749893188477, Validation Loss: 1.6000733375549316\n",
      "Epoch 73, Training Loss: 1.5759824514389038, Validation Loss: 1.595231294631958\n",
      "Epoch 74, Training Loss: 1.5713765621185303, Validation Loss: 1.5906702280044556\n",
      "Epoch 75, Training Loss: 1.5668303966522217, Validation Loss: 1.5866228342056274\n",
      "Epoch 76, Training Loss: 1.5623496770858765, Validation Loss: 1.582956075668335\n",
      "Epoch 77, Training Loss: 1.5579105615615845, Validation Loss: 1.5793834924697876\n",
      "Epoch 78, Training Loss: 1.5535742044448853, Validation Loss: 1.5758811235427856\n",
      "Epoch 79, Training Loss: 1.5492602586746216, Validation Loss: 1.5724190473556519\n",
      "Epoch 80, Training Loss: 1.545080304145813, Validation Loss: 1.5691702365875244\n",
      "Epoch 81, Training Loss: 1.5410871505737305, Validation Loss: 1.5663518905639648\n",
      "Epoch 82, Training Loss: 1.5375065803527832, Validation Loss: 1.5638999938964844\n",
      "Epoch 83, Training Loss: 1.5339299440383911, Validation Loss: 1.5614756345748901\n",
      "Epoch 84, Training Loss: 1.5303350687026978, Validation Loss: 1.5590691566467285\n",
      "Epoch 85, Training Loss: 1.5269856452941895, Validation Loss: 1.556443214416504\n",
      "Epoch 86, Training Loss: 1.5235929489135742, Validation Loss: 1.553599238395691\n",
      "Epoch 87, Training Loss: 1.5202676057815552, Validation Loss: 1.5505822896957397\n",
      "Epoch 88, Training Loss: 1.5171631574630737, Validation Loss: 1.547715663909912\n",
      "Epoch 89, Training Loss: 1.514201045036316, Validation Loss: 1.5450530052185059\n",
      "Epoch 90, Training Loss: 1.5112402439117432, Validation Loss: 1.5427515506744385\n",
      "Epoch 91, Training Loss: 1.5083214044570923, Validation Loss: 1.5407555103302002\n",
      "Epoch 92, Training Loss: 1.5055434703826904, Validation Loss: 1.5388325452804565\n",
      "Epoch 93, Training Loss: 1.5029629468917847, Validation Loss: 1.5368454456329346\n",
      "Epoch 94, Training Loss: 1.5004388093948364, Validation Loss: 1.5345762968063354\n",
      "Epoch 95, Training Loss: 1.4978338479995728, Validation Loss: 1.5322864055633545\n",
      "Epoch 96, Training Loss: 1.4952079057693481, Validation Loss: 1.5299314260482788\n",
      "Epoch 97, Training Loss: 1.4925955533981323, Validation Loss: 1.5277997255325317\n",
      "Epoch 98, Training Loss: 1.4900730848312378, Validation Loss: 1.5259305238723755\n",
      "Epoch 99, Training Loss: 1.4876450300216675, Validation Loss: 1.5239930152893066\n",
      "Epoch 100, Training Loss: 1.485275387763977, Validation Loss: 1.521946668624878\n",
      "Epoch 101, Training Loss: 1.4828118085861206, Validation Loss: 1.5195331573486328\n",
      "Epoch 102, Training Loss: 1.480219841003418, Validation Loss: 1.5169214010238647\n",
      "Epoch 103, Training Loss: 1.4776332378387451, Validation Loss: 1.514535903930664\n",
      "Epoch 104, Training Loss: 1.475095272064209, Validation Loss: 1.512215495109558\n",
      "Epoch 105, Training Loss: 1.4725853204727173, Validation Loss: 1.5101755857467651\n",
      "Epoch 106, Training Loss: 1.4701299667358398, Validation Loss: 1.5084985494613647\n",
      "Epoch 107, Training Loss: 1.4677647352218628, Validation Loss: 1.507120132446289\n",
      "Epoch 108, Training Loss: 1.465561032295227, Validation Loss: 1.505924105644226\n",
      "Epoch 109, Training Loss: 1.4635597467422485, Validation Loss: 1.5048108100891113\n",
      "Epoch 110, Training Loss: 1.4617886543273926, Validation Loss: 1.5033689737319946\n",
      "Epoch 111, Training Loss: 1.4600410461425781, Validation Loss: 1.50184965133667\n",
      "Epoch 112, Training Loss: 1.4582114219665527, Validation Loss: 1.5004512071609497\n",
      "Epoch 113, Training Loss: 1.4565201997756958, Validation Loss: 1.4993867874145508\n",
      "Epoch 114, Training Loss: 1.4549628496170044, Validation Loss: 1.498697280883789\n",
      "Epoch 115, Training Loss: 1.4534897804260254, Validation Loss: 1.4977351427078247\n",
      "Epoch 116, Training Loss: 1.4520004987716675, Validation Loss: 1.4968717098236084\n",
      "Epoch 117, Training Loss: 1.4507083892822266, Validation Loss: 1.4961987733840942\n",
      "Epoch 118, Training Loss: 1.4494879245758057, Validation Loss: 1.4951977729797363\n",
      "Epoch 119, Training Loss: 1.4481899738311768, Validation Loss: 1.4939888715744019\n",
      "Epoch 120, Training Loss: 1.4469969272613525, Validation Loss: 1.4928436279296875\n",
      "Epoch 121, Training Loss: 1.4458893537521362, Validation Loss: 1.491363286972046\n",
      "Epoch 122, Training Loss: 1.4448472261428833, Validation Loss: 1.4898426532745361\n",
      "Epoch 123, Training Loss: 1.4437795877456665, Validation Loss: 1.488561987876892\n",
      "Epoch 124, Training Loss: 1.4428150653839111, Validation Loss: 1.4877580404281616\n",
      "Epoch 125, Training Loss: 1.4418660402297974, Validation Loss: 1.4870326519012451\n",
      "Epoch 126, Training Loss: 1.4409513473510742, Validation Loss: 1.485969066619873\n",
      "Epoch 127, Training Loss: 1.4401230812072754, Validation Loss: 1.4850554466247559\n",
      "Epoch 128, Training Loss: 1.439225673675537, Validation Loss: 1.484058141708374\n",
      "Epoch 129, Training Loss: 1.4384472370147705, Validation Loss: 1.4829167127609253\n",
      "Epoch 130, Training Loss: 1.4377118349075317, Validation Loss: 1.4826141595840454\n",
      "Epoch 131, Training Loss: 1.4369447231292725, Validation Loss: 1.4825379848480225\n",
      "Epoch 132, Training Loss: 1.4363027811050415, Validation Loss: 1.482123613357544\n",
      "Epoch 133, Training Loss: 1.4356844425201416, Validation Loss: 1.4817759990692139\n",
      "Epoch 134, Training Loss: 1.4350550174713135, Validation Loss: 1.4811537265777588\n",
      "Epoch 135, Training Loss: 1.434474229812622, Validation Loss: 1.4804896116256714\n",
      "Epoch 136, Training Loss: 1.433924913406372, Validation Loss: 1.4801831245422363\n",
      "Epoch 137, Training Loss: 1.4334025382995605, Validation Loss: 1.4802556037902832\n",
      "Epoch 138, Training Loss: 1.432905673980713, Validation Loss: 1.480137825012207\n",
      "Epoch 139, Training Loss: 1.4323949813842773, Validation Loss: 1.4797543287277222\n",
      "Epoch 140, Training Loss: 1.4319149255752563, Validation Loss: 1.479483962059021\n",
      "Epoch 141, Training Loss: 1.431469440460205, Validation Loss: 1.479172945022583\n",
      "Epoch 142, Training Loss: 1.4310551881790161, Validation Loss: 1.4787031412124634\n",
      "Epoch 143, Training Loss: 1.4306758642196655, Validation Loss: 1.4784342050552368\n",
      "Epoch 144, Training Loss: 1.430296540260315, Validation Loss: 1.4780855178833008\n",
      "Epoch 145, Training Loss: 1.4299155473709106, Validation Loss: 1.4781951904296875\n",
      "Epoch 146, Training Loss: 1.4295415878295898, Validation Loss: 1.4787063598632812\n",
      "Epoch 147, Training Loss: 1.4291998147964478, Validation Loss: 1.4789996147155762\n",
      "Epoch 148, Training Loss: 1.428892731666565, Validation Loss: 1.4790347814559937\n",
      "Epoch 149, Training Loss: 1.4285733699798584, Validation Loss: 1.478636622428894\n",
      "Epoch 150, Training Loss: 1.4282495975494385, Validation Loss: 1.478036880493164\n",
      "Epoch 151, Training Loss: 1.427963137626648, Validation Loss: 1.477936863899231\n",
      "Epoch 152, Training Loss: 1.4277138710021973, Validation Loss: 1.4774500131607056\n",
      "Epoch 153, Training Loss: 1.4274839162826538, Validation Loss: 1.477797508239746\n",
      "Epoch 154, Training Loss: 1.427154302597046, Validation Loss: 1.4776408672332764\n",
      "Epoch 155, Training Loss: 1.4268760681152344, Validation Loss: 1.4774577617645264\n",
      "Epoch 156, Training Loss: 1.426650881767273, Validation Loss: 1.4776285886764526\n",
      "Epoch 157, Training Loss: 1.426440715789795, Validation Loss: 1.4771462678909302\n",
      "Epoch 158, Training Loss: 1.426290512084961, Validation Loss: 1.4774460792541504\n",
      "Epoch 159, Training Loss: 1.4260510206222534, Validation Loss: 1.4773447513580322\n",
      "Epoch 160, Training Loss: 1.4257886409759521, Validation Loss: 1.4778043031692505\n",
      "Epoch 161, Training Loss: 1.425601840019226, Validation Loss: 1.4777679443359375\n",
      "Epoch 162, Training Loss: 1.4253735542297363, Validation Loss: 1.477655053138733\n",
      "Epoch 163, Training Loss: 1.4251638650894165, Validation Loss: 1.4772762060165405\n",
      "Epoch 164, Training Loss: 1.4250036478042603, Validation Loss: 1.477107048034668\n",
      "Epoch 165, Training Loss: 1.425044298171997, Validation Loss: 1.4774858951568604\n",
      "Epoch 166, Training Loss: 1.4252045154571533, Validation Loss: 1.4765955209732056\n",
      "Epoch 167, Training Loss: 1.4246422052383423, Validation Loss: 1.4765775203704834\n",
      "Epoch 168, Training Loss: 1.42435884475708, Validation Loss: 1.47714364528656\n",
      "Epoch 169, Training Loss: 1.4243685007095337, Validation Loss: 1.4774068593978882\n",
      "Epoch 170, Training Loss: 1.424383521080017, Validation Loss: 1.4774847030639648\n",
      "Epoch 171, Training Loss: 1.4242639541625977, Validation Loss: 1.4766520261764526\n",
      "Epoch 172, Training Loss: 1.4238449335098267, Validation Loss: 1.4764742851257324\n",
      "Epoch 173, Training Loss: 1.4237604141235352, Validation Loss: 1.477156400680542\n",
      "Epoch 174, Training Loss: 1.4240373373031616, Validation Loss: 1.4763857126235962\n",
      "Epoch 175, Training Loss: 1.4234797954559326, Validation Loss: 1.476523518562317\n",
      "Epoch 176, Training Loss: 1.4233310222625732, Validation Loss: 1.4772034883499146\n",
      "Epoch 177, Training Loss: 1.423393726348877, Validation Loss: 1.476962685585022\n",
      "Epoch 178, Training Loss: 1.4231574535369873, Validation Loss: 1.4767042398452759\n",
      "Epoch 179, Training Loss: 1.4229787588119507, Validation Loss: 1.4764820337295532\n",
      "Epoch 180, Training Loss: 1.4228776693344116, Validation Loss: 1.4764310121536255\n",
      "Epoch 181, Training Loss: 1.4228273630142212, Validation Loss: 1.4767813682556152\n",
      "Epoch 182, Training Loss: 1.422903060913086, Validation Loss: 1.4764422178268433\n",
      "Epoch 183, Training Loss: 1.4227303266525269, Validation Loss: 1.4764482975006104\n",
      "Epoch 184, Training Loss: 1.4225707054138184, Validation Loss: 1.4761918783187866\n",
      "Epoch 185, Training Loss: 1.4224075078964233, Validation Loss: 1.476298213005066\n",
      "Epoch 186, Training Loss: 1.4223064184188843, Validation Loss: 1.4765267372131348\n",
      "Epoch 187, Training Loss: 1.4222652912139893, Validation Loss: 1.4764059782028198\n",
      "Epoch 188, Training Loss: 1.4223047494888306, Validation Loss: 1.476599931716919\n",
      "Epoch 189, Training Loss: 1.4223206043243408, Validation Loss: 1.4758367538452148\n",
      "Epoch 190, Training Loss: 1.4219661951065063, Validation Loss: 1.4757680892944336\n",
      "Epoch 191, Training Loss: 1.4218882322311401, Validation Loss: 1.4764312505722046\n",
      "Epoch 192, Training Loss: 1.4220190048217773, Validation Loss: 1.4760847091674805\n",
      "Epoch 193, Training Loss: 1.4218418598175049, Validation Loss: 1.476205825805664\n",
      "Epoch 194, Training Loss: 1.421609878540039, Validation Loss: 1.4759232997894287\n",
      "Epoch 195, Training Loss: 1.4214946031570435, Validation Loss: 1.4754480123519897\n",
      "Epoch 196, Training Loss: 1.421460509300232, Validation Loss: 1.475396752357483\n",
      "Epoch 197, Training Loss: 1.4214317798614502, Validation Loss: 1.475331425666809\n",
      "Epoch 198, Training Loss: 1.4213604927062988, Validation Loss: 1.475642204284668\n",
      "Epoch 199, Training Loss: 1.4212977886199951, Validation Loss: 1.475123643875122\n",
      "Epoch 200, Training Loss: 1.4211338758468628, Validation Loss: 1.4750086069107056\n",
      "Epoch 201, Training Loss: 1.421047568321228, Validation Loss: 1.4749571084976196\n",
      "Epoch 202, Training Loss: 1.4209527969360352, Validation Loss: 1.4751101732254028\n",
      "Epoch 203, Training Loss: 1.420877456665039, Validation Loss: 1.4750779867172241\n",
      "Epoch 204, Training Loss: 1.4208234548568726, Validation Loss: 1.4748871326446533\n",
      "Epoch 205, Training Loss: 1.4207372665405273, Validation Loss: 1.474652886390686\n",
      "Epoch 206, Training Loss: 1.4206757545471191, Validation Loss: 1.4744808673858643\n",
      "Epoch 207, Training Loss: 1.4206066131591797, Validation Loss: 1.4745399951934814\n",
      "Epoch 208, Training Loss: 1.4205330610275269, Validation Loss: 1.4745904207229614\n",
      "Epoch 209, Training Loss: 1.4205191135406494, Validation Loss: 1.4746794700622559\n",
      "Epoch 210, Training Loss: 1.420670747756958, Validation Loss: 1.4736177921295166\n",
      "Epoch 211, Training Loss: 1.4206148386001587, Validation Loss: 1.473907709121704\n",
      "Epoch 212, Training Loss: 1.4204813241958618, Validation Loss: 1.4736218452453613\n",
      "Epoch 213, Training Loss: 1.4202721118927002, Validation Loss: 1.4737364053726196\n",
      "Epoch 214, Training Loss: 1.420408010482788, Validation Loss: 1.4751582145690918\n",
      "Epoch 215, Training Loss: 1.4207451343536377, Validation Loss: 1.4735227823257446\n",
      "Epoch 216, Training Loss: 1.4201444387435913, Validation Loss: 1.4733716249465942\n",
      "Epoch 217, Training Loss: 1.4203953742980957, Validation Loss: 1.47478449344635\n",
      "Epoch 218, Training Loss: 1.4207468032836914, Validation Loss: 1.472894310951233\n",
      "Epoch 219, Training Loss: 1.4199748039245605, Validation Loss: 1.473109483718872\n",
      "Epoch 220, Training Loss: 1.4204881191253662, Validation Loss: 1.4753572940826416\n",
      "Epoch 221, Training Loss: 1.4206429719924927, Validation Loss: 1.4735208749771118\n",
      "Epoch 222, Training Loss: 1.4198020696640015, Validation Loss: 1.4729360342025757\n",
      "Epoch 223, Training Loss: 1.4202839136123657, Validation Loss: 1.4738878011703491\n",
      "Epoch 224, Training Loss: 1.4198803901672363, Validation Loss: 1.4736031293869019\n",
      "Epoch 225, Training Loss: 1.419838786125183, Validation Loss: 1.47243332862854\n",
      "Epoch 226, Training Loss: 1.4199612140655518, Validation Loss: 1.4729009866714478\n",
      "Epoch 227, Training Loss: 1.4194912910461426, Validation Loss: 1.474288821220398\n",
      "Epoch 228, Training Loss: 1.4198548793792725, Validation Loss: 1.4726883172988892\n",
      "Epoch 229, Training Loss: 1.419480323791504, Validation Loss: 1.4724175930023193\n",
      "Epoch 230, Training Loss: 1.419521689414978, Validation Loss: 1.4733591079711914\n",
      "Epoch 231, Training Loss: 1.4196832180023193, Validation Loss: 1.4723581075668335\n",
      "Epoch 232, Training Loss: 1.4192396402359009, Validation Loss: 1.4721152782440186\n",
      "Epoch 233, Training Loss: 1.419595718383789, Validation Loss: 1.4728267192840576\n",
      "Epoch 234, Training Loss: 1.419374704360962, Validation Loss: 1.4725909233093262\n",
      "Epoch 235, Training Loss: 1.4191844463348389, Validation Loss: 1.4719250202178955\n",
      "Epoch 236, Training Loss: 1.419399380683899, Validation Loss: 1.472010850906372\n",
      "Epoch 237, Training Loss: 1.4190027713775635, Validation Loss: 1.4719144105911255\n",
      "Epoch 238, Training Loss: 1.4190694093704224, Validation Loss: 1.4712634086608887\n",
      "Epoch 239, Training Loss: 1.418985366821289, Validation Loss: 1.471787452697754\n",
      "Epoch 240, Training Loss: 1.4187729358673096, Validation Loss: 1.4723728895187378\n",
      "Epoch 241, Training Loss: 1.4188294410705566, Validation Loss: 1.4715816974639893\n",
      "Epoch 242, Training Loss: 1.4186725616455078, Validation Loss: 1.471301555633545\n",
      "Epoch 243, Training Loss: 1.4186596870422363, Validation Loss: 1.4714709520339966\n",
      "Epoch 244, Training Loss: 1.4186309576034546, Validation Loss: 1.470896601676941\n",
      "Epoch 245, Training Loss: 1.418476939201355, Validation Loss: 1.470758318901062\n",
      "Epoch 246, Training Loss: 1.418457269668579, Validation Loss: 1.4714466333389282\n",
      "Epoch 247, Training Loss: 1.4184869527816772, Validation Loss: 1.4707564115524292\n",
      "Epoch 248, Training Loss: 1.4183205366134644, Validation Loss: 1.4703246355056763\n",
      "Epoch 249, Training Loss: 1.418251395225525, Validation Loss: 1.470611572265625\n",
      "Epoch 250, Training Loss: 1.418284296989441, Validation Loss: 1.470561146736145\n",
      "Epoch 251, Training Loss: 1.4182027578353882, Validation Loss: 1.4706690311431885\n",
      "Epoch 252, Training Loss: 1.4180970191955566, Validation Loss: 1.470635175704956\n",
      "Epoch 253, Training Loss: 1.4181543588638306, Validation Loss: 1.4701231718063354\n",
      "Epoch 254, Training Loss: 1.4180395603179932, Validation Loss: 1.4698073863983154\n",
      "Epoch 255, Training Loss: 1.4179797172546387, Validation Loss: 1.469111680984497\n",
      "Epoch 256, Training Loss: 1.417962908744812, Validation Loss: 1.46880304813385\n",
      "Epoch 257, Training Loss: 1.4179223775863647, Validation Loss: 1.4692214727401733\n",
      "Epoch 258, Training Loss: 1.4178231954574585, Validation Loss: 1.4691575765609741\n",
      "Epoch 259, Training Loss: 1.417824625968933, Validation Loss: 1.4687200784683228\n",
      "Epoch 260, Training Loss: 1.4177098274230957, Validation Loss: 1.4683655500411987\n",
      "Epoch 261, Training Loss: 1.4176428318023682, Validation Loss: 1.4679701328277588\n",
      "Epoch 262, Training Loss: 1.4176639318466187, Validation Loss: 1.4679287672042847\n",
      "Epoch 263, Training Loss: 1.4175646305084229, Validation Loss: 1.468087911605835\n",
      "Epoch 264, Training Loss: 1.417464017868042, Validation Loss: 1.4687734842300415\n",
      "Epoch 265, Training Loss: 1.4174493551254272, Validation Loss: 1.4688836336135864\n",
      "Epoch 266, Training Loss: 1.4173787832260132, Validation Loss: 1.4686628580093384\n",
      "Epoch 267, Training Loss: 1.417283058166504, Validation Loss: 1.468284010887146\n",
      "Epoch 268, Training Loss: 1.4172658920288086, Validation Loss: 1.4681566953659058\n",
      "Epoch 269, Training Loss: 1.417221188545227, Validation Loss: 1.468234658241272\n",
      "Epoch 270, Training Loss: 1.4171514511108398, Validation Loss: 1.4684184789657593\n",
      "Epoch 271, Training Loss: 1.4171252250671387, Validation Loss: 1.4686658382415771\n",
      "Epoch 272, Training Loss: 1.4170875549316406, Validation Loss: 1.4685425758361816\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "optimizer = torch.optim.Adam(dml_model.parameters(), lr=0.01)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dml_model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    outputs = dml_model(user_features, all_x_included_products, all_x_other_products,all_bundle_prices)[0]\n",
    "    choice_probabilities = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "    loss = -torch.mean(choice_probabilities[torch.arange(choice_probabilities.shape[0]), decision_train1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    dml_model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = dml_model(X_user_val,  all_x_included_products, all_x_other_products ,all_bundle_prices)[0]\n",
    "        val_choice_probabilities = F.log_softmax(val_outputs, dim=1)\n",
    "        val_loss = -torch.mean(val_choice_probabilities[torch.arange(val_choice_probabilities.shape[0]),decision_val])\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "    # Check if validation loss improved\n",
    "    if (val_loss < best_val_loss)|(val_loss<loss):\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter on improvement\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b27e55e",
   "metadata": {
    "id": "1b27e55e"
   },
   "outputs": [],
   "source": [
    "#for complementarity model\n",
    "def calculate_expected_revenue(model, user_features, all_x_included_products, bundle_prices):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Calculate utilities for all bundles\n",
    "        utilities = model(user_features, all_x_included_products, all_x_other_products,bundle_prices)[0]\n",
    "        probabilities = F.softmax(utilities, dim=1)\n",
    "\n",
    "        # Calculate total expected revenue\n",
    "        total_expected_revenue = (probabilities * bundle_prices.unsqueeze(0)).sum()\n",
    "\n",
    "    return total_expected_revenue.item()  # Convert to Python float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5efd256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Revenue all Control: $10135.72\n",
      "Expected Revenue all treated: $1638.34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8497.37890625"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "X_user_test, X_product, price = X_user_test.to(device), X_product.to(device), price.to(device)\n",
    "control_prepared_data = prepare_data_bundle(X_user_test, X_product,  price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = control_prepared_data\n",
    "expected_revenue_all_control = calculate_expected_revenue(dml_model, user_features, all_x_included_products, all_bundle_prices)\n",
    "print(f\"Expected Revenue all Control: ${expected_revenue_all_control:.2f}\")\n",
    "\n",
    "all_treated_price = price*discount\n",
    "treated_prepared_data = prepare_data_bundle(X_user_test, X_product,  all_treated_price)\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = treated_prepared_data\n",
    "expected_revenue_all_treated = calculate_expected_revenue(dml_model, user_features, all_x_included_products, all_bundle_prices)\n",
    "print(f\"Expected Revenue all treated: ${expected_revenue_all_treated:.2f}\")\n",
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "QGABODM51OV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGABODM51OV4",
    "outputId": "3769a33f-2282-4787-e276-3f7d3b56c540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8497.37890625"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_revenue_all_treated-expected_revenue_all_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121cd47",
   "metadata": {
    "id": "1121cd47"
   },
   "source": [
    "# debias the GTE estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "SMTzkngzuUls",
   "metadata": {
    "id": "SMTzkngzuUls"
   },
   "outputs": [],
   "source": [
    "test_prepared_data = prepare_data_bundle(X_user_test, X_product,  price*(1-(1-discount)*prod_randomization))\n",
    "user_features, product_features, prices, bundle_choices, all_x_included_products, all_x_other_products, all_bundle_prices = test_prepared_data\n",
    "\n",
    "# Compute Theta0 and Theta1\n",
    "_,theta0_output,theta1_output = dml_model(user_features, all_x_included_products, all_x_other_products,all_bundle_prices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rag3u55FWjiu",
   "metadata": {
    "id": "Rag3u55FWjiu"
   },
   "source": [
    "# use formulation debias for H_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "SSXrdFP4WnZL",
   "metadata": {
    "id": "SSXrdFP4WnZL"
   },
   "outputs": [],
   "source": [
    "def H_theta(theta0_output,theta1_output,all_treated_price,price):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = 2**NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = all_treated_price.unsqueeze(0).expand(N, M)\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Include the outside option (utility = 0)\n",
    "    zero_utilities = torch.zeros(N, 1, device=all_treated_uti.device)\n",
    "    all_treated_uti = torch.cat((zero_utilities,all_treated_uti), dim=1)\n",
    "    all_control_uti = torch.cat((zero_utilities,all_control_uti), dim=1)\n",
    "\n",
    "    all_treated_probabilities = F.softmax(all_treated_uti, dim=1)\n",
    "    all_control_probabilities = F.softmax(all_control_uti, dim=1)\n",
    "\n",
    "    price_with_outside = torch.cat((torch.zeros(1, device=price.device),price), dim=0)\n",
    "    treated_price_with_outside =  torch.cat((torch.zeros(1, device=all_treated_price.device),all_treated_price), dim=0)\n",
    "\n",
    "    H = torch.sum(all_treated_probabilities*treated_price_with_outside - all_control_probabilities*price_with_outside,dim=1)\n",
    "    expsum_treated = torch.sum(torch.exp(all_treated_uti),dim=1)\n",
    "    expsum_control = torch.sum(torch.exp(all_control_uti),dim=1)\n",
    "\n",
    "    expsum_treated_expanded = expsum_treated.unsqueeze(1).expand(-1, all_treated_uti.shape[1])  # Shape [N, M+1]\n",
    "    expsum_control_expanded = expsum_control.unsqueeze(1).expand(-1, all_control_uti.shape[1])  # Shape [N, M+1]\n",
    "\n",
    "    H_theta0 = torch.sum((torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded-\\\n",
    "                          torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded)\\\n",
    "                         *price_with_outside,dim=1)\n",
    "    H_theta1 = torch.sum(price_with_outside*(torch.exp(all_treated_uti)*(1-torch.exp(all_treated_uti))/expsum_treated_expanded/expsum_treated_expanded*treated_price_with_outside-\\\n",
    "                                             torch.exp(all_control_uti)*(1-torch.exp(all_control_uti))/expsum_control_expanded/expsum_control_expanded*price_with_outside),dim=1)\n",
    "\n",
    "\n",
    "    return H,H_theta0,H_theta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "776a374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_treated_price = price*discount\n",
    "all_treated_bundle_price = torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    all_treated_bundle_price[b] = torch.sum(all_treated_price[bundle_mask.bool()])\n",
    "all_bundle_prices= torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    all_bundle_prices[b] = torch.sum(price[bundle_mask.bool()])\n",
    "H,H_theta0,H_theta1 = H_theta(theta0_output,theta1_output,all_treated_bundle_price,all_bundle_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "yivIC_MKad5x",
   "metadata": {
    "id": "yivIC_MKad5x"
   },
   "outputs": [],
   "source": [
    "def l_theta(theta0_output,theta1_output,adjusted_price,decision_test):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = 2**NUM_Product\n",
    "    expand_adjusted_price = adjusted_price.unsqueeze(0).expand(N, M)\n",
    "    uti = theta0_output + theta1_output * expand_adjusted_price\n",
    "    adjusted_price_with_outside =  torch.cat([torch.zeros(1, device=adjusted_price.device),adjusted_price])\n",
    "\n",
    "    probabilities = F.softmax(uti, dim=1)\n",
    "    prod_indices = torch.ones(2**NUM_Product, device=device)\n",
    "    ltheta0 = probabilities[torch.arange(decision_test.size(0)), decision_test] -prod_indices[decision_test]\n",
    "    ltheta1 = (probabilities[torch.arange(decision_test.size(0)), decision_test] * adjusted_price_with_outside[decision_test]) - adjusted_price_with_outside[decision_test]\n",
    "\n",
    "\n",
    "    return ltheta0,ltheta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77bcb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "O8c-tupIgHVu",
   "metadata": {
    "id": "O8c-tupIgHVu"
   },
   "outputs": [],
   "source": [
    "adjusted_price = price*(1-(1-discount)*prod_randomization).to(device)\n",
    "adjusted_bundle_price = torch.zeros(2**NUM_Product, device=device)\n",
    "for b in range(2**NUM_Product):\n",
    "    bundle_mask = bundle_choices[b]\n",
    "    adjusted_bundle_price[b] = torch.sum(adjusted_price[bundle_mask.bool()])\n",
    "decision_test = decision_test.to(device)\n",
    "ltheta0,ltheta1= l_theta(theta0_output,theta1_output,adjusted_bundle_price,decision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "UVRht78QaxSG",
   "metadata": {
    "id": "UVRht78QaxSG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def lambdainv(theta0_output, theta1_output, price, decision_test,epsilon =10):\n",
    "    N = theta0_output.shape[0]\n",
    "    M = 2**NUM_Product\n",
    "    expand_price = price.unsqueeze(0).expand(N, M)\n",
    "    expand_all_treated_price = discount*price.unsqueeze(0).expand(N, M)\n",
    "\n",
    "    all_treated_uti = theta0_output + theta1_output * expand_all_treated_price\n",
    "    all_control_uti =  theta0_output + theta1_output * expand_price\n",
    "\n",
    "\n",
    "    # Calculate probabilities using softmax\n",
    "    probabilities_control = F.softmax(all_control_uti, dim=1)\n",
    "    probabilities_treated = F.softmax(all_treated_uti, dim=1)\n",
    "\n",
    "    # Extract probabilities of chosen products\n",
    "    chosen_prob_control = probabilities_control[torch.arange(N), decision_test]\n",
    "    chosen_prob_treated = probabilities_treated[torch.arange(N), decision_test]\n",
    "\n",
    "    # Calculate second derivatives\n",
    "    ltheta00 = -chosen_prob_control * (1 - chosen_prob_control) - chosen_prob_treated * (1 - chosen_prob_treated)\n",
    "    ltheta01 = -chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test] - \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])\n",
    "    ltheta11 = -chosen_prob_control * (1 - chosen_prob_control) * expand_price[torch.arange(N), decision_test]**2 - \\\n",
    "            chosen_prob_treated * (1 - chosen_prob_treated) * (discount * expand_price[torch.arange(N), decision_test])**2\n",
    "    ltheta00=ltheta00/2\n",
    "    ltheta01=ltheta01/2\n",
    "    ltheta11=ltheta11/2\n",
    "\n",
    "    # Form the 2x2 Hessian matrices for each instance\n",
    "    ltheta00 = ltheta00.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta01 = ltheta01.unsqueeze(1).unsqueeze(2)\n",
    "    ltheta11 = ltheta11.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    top_row = torch.cat((ltheta00, ltheta01), dim=2)\n",
    "    bottom_row = torch.cat((ltheta01, ltheta11), dim=2)\n",
    "\n",
    "    L_matrix = torch.cat((top_row, bottom_row), dim=1)\n",
    "\n",
    "    # Regularization and inversion\n",
    "\n",
    "    identity_matrix = torch.eye(2, dtype=L_matrix.dtype, device=L_matrix.device) * epsilon\n",
    "    L_matrix_reg = L_matrix + identity_matrix.unsqueeze(0).unsqueeze(0)\n",
    "    L_inv = torch.linalg.inv(L_matrix_reg)\n",
    "\n",
    "    return L_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6f7441c-eea7-4f98-8631-71cb75d14600",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0.001,0.01,0.1,0.5,1,5,10]\n",
    "min_mape = float('inf')\n",
    "best_epsilon = None\n",
    "best_final_result = None\n",
    "\n",
    "for epsilon in epsilon_list:\n",
    "    # Update L_inv for the current epsilon\n",
    "    try:\n",
    "        L_inv = lambdainv(theta0_output, theta1_output, all_bundle_prices, decision_test, epsilon).float()\n",
    "\n",
    "        # Calculate final_result with the given epsilon\n",
    "        H_theta_array = torch.stack((H_theta0, H_theta1), dim=-1).unsqueeze(1).float()\n",
    "        l_theta_array = torch.stack((ltheta0, ltheta1), dim=-1).unsqueeze(-1).float()\n",
    "\n",
    "        # Perform matrix multiplications\n",
    "        result_intermediate = torch.matmul(H_theta_array, L_inv.squeeze(0))\n",
    "        final_result = torch.matmul(result_intermediate, l_theta_array).squeeze(-1)\n",
    "        final_result[torch.isnan(final_result) | torch.isinf(final_result)] = 0\n",
    "\n",
    "        # Calculate sdl and dedl\n",
    "        sdl = H.sum().cpu().detach().numpy() * 2\n",
    "        dedl = (H.sum().cpu().detach().numpy() - final_result.sum().cpu().detach().numpy()) * 2\n",
    "\n",
    "        # Calculate MAPE of dedl with respect to true\n",
    "        mape_dedl = np.abs((dedl - true) / true)\n",
    "\n",
    "        # Update best_epsilon if the current epsilon yields a lower MAPE\n",
    "        if mape_dedl < min_mape:\n",
    "            min_mape = mape_dedl\n",
    "            best_epsilon = epsilon\n",
    "            best_final_result = final_result\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "q11HQu-goWM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q11HQu-goWM0",
    "outputId": "ecf71629-8dbc-4d80-8d4a-de1f2534e3eb"
   },
   "outputs": [],
   "source": [
    "sdl = H.sum().cpu().detach().numpy()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3a445f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedl = (H.sum().cpu().detach().numpy()-best_final_result.sum().cpu().detach().numpy())*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c6d8d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-16983.65625, -14962.857421875, 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl,dedl,best_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d44fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Estimation Error of SDL:  -11.27%\n",
      "Absolute Percentage Estimation Error of SP MNL:  -1.97%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute Percentage Estimation Error of SDL:  {100*np.abs(sdl-revenue_difference)/revenue_difference:.2f}%\")\n",
    "print(f\"Absolute Percentage Estimation Error of SP MNL:  {100*np.abs(dedl-revenue_difference)/revenue_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eeea3d7d-d20e-48c8-b037-107fce171386",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_pe = (naive - true) / true\n",
    "linear_pe = (linear - true) / true\n",
    "pdl_pe = (pdl - true) / true\n",
    "sdl_pe = (sdl - true) / true\n",
    "dedl_pe = (dedl - true) / true\n",
    "naive_mse = (naive - true)**2\n",
    "linear_mse =(linear - true)**2\n",
    "pdl_mse = (pdl - true)**2\n",
    "sdl_mse = (sdl - true)**2\n",
    "dedl_mse = (dedl - true)**2\n",
    "naive_e = (naive - true)\n",
    "linear_e =(linear - true)\n",
    "pdl_e = (pdl - true)\n",
    "sdl_e = (sdl - true)\n",
    "dedl_e = (dedl - true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1094fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0133498693682532 -0.9925782646659882 0.062019795339858574 0.1126821824789937 -0.019710208024618734 -15467.476799160242 15150.425084322691 -946.652064114809 -1719.947962552309 300.85086557269096 239242838.53256038 229535380.23567423 896150.1304928286 2958220.993887839 90511.24331583738\n"
     ]
    }
   ],
   "source": [
    "print(naive_pe,linear_pe,pdl_pe,sdl_pe,dedl_pe,naive_e,linear_e,pdl_e,sdl_e,dedl_e,naive_mse,linear_mse,pdl_mse,sdl_mse,dedl_mse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "c2a5b9ca",
    "7e135e9e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
